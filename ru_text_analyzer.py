import streamlit as st
import re
import os
import json
import pandas as pd
from pymystem3 import Mystem
from google import genai
from google.cloud import vision
import io
import urllib.parse
from typing import Union

# ---------------------- 0. ì´ˆê¸° ì„¤ì • ë° ìƒìˆ˜ ----------------------

NEW_DEFAULT_TEXT = """Ğ¢Ğ¾Ğ¼ Ğ¶Ğ¸Ğ²Ñ‘Ñ‚ Ğ² Ğ¡Ğ°Ğ½ĞºÑ‚-ĞŸĞµÑ‚ĞµÑ€Ğ±ÑƒÑ€Ğ³Ğµ ÑƒĞ¶Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼ĞµÑÑÑ†ĞµĞ². Ğ’ ÑÑƒĞ±Ğ±Ğ¾Ñ‚Ñƒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ±Ñ‹Ğ»Ğ° Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ°Ñ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ°, Ğ¢Ğ¾Ğ¼ Ñ€ĞµÑˆĞ¸Ğ» Ğ¿Ğ¾Ğ¹Ñ‚Ğ¸ Ğ² Ğ˜ÑĞ°Ğ°ĞºĞ¸ĞµĞ²ÑĞºĞ¸Ğ¹ ÑĞ¾Ğ±Ğ¾Ñ€. Ğ¢Ğ¾Ğ¼ Ğ´Ğ°Ğ²Ğ½Ğ¾ Ğ¼ĞµÑ‡Ñ‚Ğ°Ğ» Ğ¿Ğ¾Ğ±Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ² ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ğ±Ğ¾Ñ€Ğµ. Ğ˜ÑĞ°Ğ°ĞºĞ¸ĞµĞ²ÑĞºĞ¸Ğ¹ ÑĞ¾Ğ±Ğ¾Ñ€ â€” Ğ¾Ğ´Ğ½Ğ¾ Ğ¸Ğ· ÑĞ°Ğ¼Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¡Ğ°Ğ½ĞºÑ‚-ĞŸĞµÑ‚ĞµÑ€Ğ±ÑƒÑ€Ğ³Ğµ, ĞµĞ³Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ²Ğ¸Ğ´ĞµÑ‚ÑŒ
Ğ´Ğ°Ğ¶Ğµ Ğ¸Ğ·Ğ´Ğ°Ğ»ĞµĞºĞ°. ĞšĞ¾Ğ³Ğ´Ğ° Ğ¢Ğ¾Ğ¼ Ğ³ÑƒĞ»ÑĞ» Ğ¿Ğ¾ Ñ†ĞµĞ½Ñ‚Ñ€Ñƒ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°, Ğ¾Ğ½ Ğ¾Ñ‚Ğ¾Ğ²ÑÑĞ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ» Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ ĞºÑƒĞ¿Ğ¾Ğ» ÑĞ¾Ğ±Ğ¾Ñ€Ğ°. Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¢Ğ¾Ğ¼ Ñ€ĞµÑˆĞ¸Ğ» Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ ÑĞ¾Ğ±Ğ¾Ñ€ ÑĞ½Ğ°Ñ€ÑƒĞ¶Ğ¸. ĞĞ½ Ğ¿Ñ€Ğ¸ÑˆÑ‘Ğ» Ğ½Ğ° Ğ˜ÑĞ°Ğ°ĞºĞ¸ĞµĞ²ÑĞºÑƒÑ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ â€” Ğ¾Ñ‚ÑÑĞ´Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞºÑ€Ğ°ÑĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ Ğ½Ğ° ÑĞ¾Ğ±Ğ¾Ñ€. ĞŸĞ¾Ñ‚Ğ¾Ğ¼ Ğ¢Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ¾ÑˆÑ‘Ğ» Ğº ÑĞ¾Ğ±Ğ¾Ñ€Ñƒ Ğ¿Ğ¾Ğ±Ğ»Ğ¸Ğ¶Ğµ, Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ» ĞµĞ³Ğ¾ ÑĞ¿ĞµÑ€ĞµĞ´Ğ¸, ÑĞ·Ğ°Ğ´Ğ¸, 2 Ñ€Ğ°Ğ·Ğ° Ğ¾Ğ±Ğ¾ÑˆÑ‘Ğ» Ğ²Ğ¾ĞºÑ€ÑƒĞ³ ÑĞ¾Ğ±Ğ¾Ñ€Ğ°, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ğ²Ğ¾ÑˆÑ‘Ğ» Ğ²Ğ½ÑƒÑ‚Ñ€ÑŒ. Ğ’Ğ½ÑƒÑ‚Ñ€Ğ¸ ÑĞ¾Ğ±Ğ¾Ñ€ Ğ¾Ñ‡ĞµĞ½ÑŒ ĞºÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğ¹. Ğ¢Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ», Ñ‡Ñ‚Ğ¾ ĞºÑƒĞ¿Ğ¾Ğ» ÑĞ¾Ğ±Ğ¾Ñ€Ğ° â€” Ñ‚Ñ€ĞµÑ‚Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ğµ Ğ² Ğ•Ğ²Ñ€Ğ¾Ğ¿Ğµ. Ğ¢Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ½ÑĞ» Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñƒ Ğ²Ğ²ĞµÑ€Ñ… Ğ¸ ÑƒĞ²Ğ¸Ğ´ĞµĞ», Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´ ĞºÑƒĞ¿Ğ¾Ğ»Ğ¾Ğ¼ Â«Ğ»ĞµÑ‚Ğ°ĞµÑ‚Â» ÑĞµÑ€ĞµĞ±Ñ€ÑĞ½Ñ‹Ğ¹ Ğ³Ğ¾Ğ»ÑƒĞ±ÑŒ. Ğ¢Ğ¾Ğ¼ Ğ¿Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ» Ğ²Ğ¾ĞºÑ€ÑƒĞ³: Ğ²Ğ¿ĞµÑ€ĞµĞ´Ğ¸, ÑĞ·Ğ°Ğ´Ğ¸, ÑĞ¿Ñ€Ğ°Ğ²Ğ°, ÑĞ»ĞµĞ²Ğ° â€” Ğ²ĞµĞ·Ğ´Ğµ Ğ±Ñ‹Ğ»Ğ¸ ĞºÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğµ Ğ¸ĞºĞ¾Ğ½Ñ‹.
ĞŸĞ¾Ñ‚Ğ¾Ğ¼ Ğ¢Ğ¾Ğ¼ Ñ€ĞµÑˆĞ¸Ğ» Ğ¿Ğ¾Ğ´Ğ½ÑÑ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ»Ğ¾Ğ½Ğ½Ğ°Ğ´Ñƒ ÑĞ¾Ğ±Ğ¾Ñ€Ğ°. Ğ’ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ´ĞµĞ½ÑŒ Ğ² ÑĞ¾Ğ±Ğ¾Ñ€Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ÑƒÑ€Ğ¸ÑÑ‚Ğ¾Ğ²: Ğ¾Ğ´Ğ½Ğ¸ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑŒ Ğ²Ğ²ĞµÑ€Ñ…, Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¿ÑƒÑĞºĞ°Ğ»Ğ¸ÑÑŒ Ğ²Ğ½Ğ¸Ğ·. Ğ¢Ğ¾Ğ¼ Ñ‚Ğ¾Ğ¶Ğµ Ğ¿Ğ¾Ğ´Ğ½ÑĞ»ÑÑ Ğ²Ğ²ĞµÑ€Ñ…. ĞÑ‚Ñ‚ÑƒĞ´Ğ°, ÑĞ²ĞµÑ€Ñ…Ñƒ, Ñ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ 43 (ÑĞ¾Ñ€Ğ¾ĞºĞ° Ñ‚Ñ€Ñ‘Ñ…) Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞºÑ€Ğ°ÑĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ Ğ½Ğ° Ñ†ĞµĞ½Ñ‚Ñ€ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°. Ğ¢Ğ¾Ğ¼ ÑƒĞ²Ğ¸Ğ´ĞµĞ» Ğ”Ğ²Ğ¾Ñ€Ñ†Ğ¾Ğ²ÑƒÑ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ, ĞŸĞµÑ‚Ñ€Ğ¾Ğ¿Ğ°Ğ²Ğ»Ğ¾Ğ²ÑĞºÑƒÑ ĞºÑ€ĞµĞ¿Ğ¾ÑÑ‚ÑŒ, ĞºÑ€Ñ‹ÑˆĞ¸ Ğ´Ğ¾Ğ¼Ğ¾Ğ², Ğ° Ğ½Ğ°Ğ´ ĞºÑ€Ñ‹ÑˆĞ°Ğ¼Ğ¸ Ğ»ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ‚Ğ¸Ñ†Ñ‹.
Ğ¢Ğ¾Ğ¼Ñƒ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°ÑÑŒ ÑĞºÑĞºÑƒÑ€ÑĞ¸Ñ. ĞĞ½ Ğ¿Ğ¾ÑĞ¾Ğ²ĞµÑ‚Ğ¾Ğ²Ğ°Ğ» Ğ´Ñ€ÑƒĞ·ÑŒÑĞ¼ Ğ¿Ğ¾ÑĞµÑ‚Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ±Ğ¾Ñ€ Ğ¸ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ½ÑÑ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ»Ğ¾Ğ½Ğ½Ğ°Ğ´Ñƒ."""

DEFAULT_TEST_TEXT = "Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¸Ğ´Ñ‘Ñ‚ Ğ¿Ğ¾ ÑƒĞ»Ğ¸Ñ†Ğµ. Ğ­Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°. Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾. Ğ¯ Ñ‡Ğ°ÑÑ‚Ğ¾ Ñ‡Ğ¸Ñ‚Ğ°Ñ ÑÑ‚Ñƒ ĞºĞ½Ğ¸Ğ³Ñƒ."

mystem = Mystem()
YOUTUBE_VIDEO_ID = "wJ65i_gDfT0"Â 
IMAGE_FILE_PATH = "banner.png"

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” í•¨ìˆ˜ (AttributeError ë°©ì§€) ---
def initialize_session_state():
Â  Â  if "selected_words" not in st.session_state:
Â  Â  Â  Â  st.session_state.selected_words = []
Â  Â  if "clicked_word" not in st.session_state:
Â  Â  Â  Â  st.session_state.clicked_word = None
Â  Â  if "word_info" not in st.session_state:
Â  Â  Â  Â  st.session_state.word_info = {}
Â  Â  if "current_search_query" not in st.session_state:
Â  Â  Â  Â  st.session_state.current_search_query = ""
Â  Â  if "ocr_output_text" not in st.session_state:
Â  Â  Â  Â  st.session_state.ocr_output_text = ""
Â  Â  if "input_text_area" not in st.session_state:
Â  Â  Â  Â  st.session_state.input_text_area = DEFAULT_TEST_TEXT
Â  Â  if "translated_text" not in st.session_state:
Â  Â  Â  Â  st.session_state.translated_text = ""
Â  Â  if "last_processed_text" not in st.session_state:
Â  Â  Â  Â  st.session_state.last_processed_text = ""
Â  Â  if "last_processed_query" not in st.session_state:
Â  Â  Â  Â  st.session_state.last_processed_query = ""

# ---------------------- 0.1. í˜ì´ì§€ ì„¤ì • ë° ë°°ë„ˆ ì‚½ì… ----------------------

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ì‹¤í–‰
initialize_session_state()

st.set_page_config(page_title="ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ê¸°", layout="wide")

try:
Â  Â  st.image(IMAGE_FILE_PATH, use_column_width=True)
except FileNotFoundError:
Â  Â  st.warning(f"ë°°ë„ˆ ì´ë¯¸ì§€ íŒŒì¼ ({IMAGE_FILE_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GitHub ì €ì¥ì†Œì— ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  íŒŒì¼ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
Â  Â  st.markdown("###")


# ---------------------- 0.2. YouTube ì„ë² ë“œ í•¨ìˆ˜ ----------------------

def youtube_embed_html(video_id: str):
Â  Â  """ì§€ì •ëœ YouTube IDë¡œ ë°˜ì‘í˜• ì„ë² ë“œ HTMLì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
Â  Â  embed_url = f"https://www.youtube.com/embed/{video_id}?autoplay=0&rel=0"

Â  Â  html_code = f"""
Â  Â  <div class="video-container-wrapper">
Â  Â  Â  Â  <div class="video-responsive">
Â  Â  Â  Â  Â  Â  <iframe
Â  Â  Â  Â  Â  Â  Â  Â  src="{embed_url}"
Â  Â  Â  Â  Â  Â  Â  Â  frameborder="0"
Â  Â  Â  Â  Â  Â  Â  Â  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
Â  Â  Â  Â  Â  Â  Â  Â  allowfullscreen
Â  Â  Â  Â  Â  Â  Â  Â  title="í”„ë¡œì íŠ¸ í™ë³´ ì˜ìƒ"
Â  Â  Â  Â  Â  Â  ></iframe>
Â  Â  Â  Â  </div>
Â  Â  </div>
Â  Â  """
Â  Â  return html_code


# ---------------------- í’ˆì‚¬ ë³€í™˜ ë”•ì…”ë„ˆë¦¬ ë° Mystem í•¨ìˆ˜ ----------------------
POS_MAP = {
Â  Â  'S': 'ëª…ì‚¬', 'V': 'ë™ì‚¬', 'A': 'í˜•ìš©ì‚¬', 'ADV': 'ë¶€ì‚¬', 'PR': 'ì „ì¹˜ì‚¬',
Â  Â  'CONJ': 'ì ‘ì†ì‚¬', 'INTJ': 'ê°íƒ„ì‚¬', 'PART': 'ë¶ˆë³€í™”ì‚¬', 'NUM': 'ìˆ˜ì‚¬',
Â  Â  'APRO': 'ëŒ€ëª…ì‚¬ì  í˜•ìš©ì‚¬', 'ANUM': 'ì„œìˆ˜ì‚¬', 'SPRO': 'ëŒ€ëª…ì‚¬',
Â  Â  'PRICL': 'ë™ì‚¬ë¶€ì‚¬',
Â  Â  'COMP': 'ë¹„êµê¸‰', 'A=cmp': 'ë¹„êµê¸‰ í˜•ìš©ì‚¬', 'ADV=cmp': 'ë¹„êµê¸‰ ë¶€ì‚¬',
Â  Â  'ADVB': 'ë¶€ì‚¬',
Â  Â  'NONLEX': 'ë¹„ë‹¨ì–´',Â  Â  Â Â 
Â  Â  'INIT': 'ë¨¸ë¦¬ê¸€ì',Â  Â  Â Â 
Â  Â  'P': 'ë¶ˆë³€í™”ì‚¬/ì „ì¹˜ì‚¬',Â 
Â  Â  'ADJ': 'í˜•ìš©ì‚¬',Â  Â  Â  Â  Â 
Â  Â  'N': 'ëª…ì‚¬',Â  Â  Â  Â  Â  Â  Â 
}

@st.cache_data(show_spinner=False)
def lemmatize_ru(word: str) -> str:
Â  Â  if ' ' in word.strip():
Â  Â  Â  Â  return word.strip()
Â  Â  if re.fullmatch(r'\w+', word, flags=re.UNICODE):
Â  Â  Â  Â  lemmas = mystem.lemmatize(word)
Â  Â  Â  Â  return (lemmas[0] if lemmas else word).strip()
Â  Â  return word

@st.cache_data(show_spinner=False)
def get_pos_ru(word: str) -> str:
Â  Â  if ' ' in word.strip():
Â  Â  Â  Â  return 'êµ¬ í˜•íƒœ'Â 
Â  Â  if re.fullmatch(r'\w+', word, flags=re.UNICODE):
Â  Â  Â  Â  analysis = mystem.analyze(word)
Â  Â  Â  Â  if analysis and 'analysis' in analysis[0] and analysis[0]['analysis']:
Â  Â  Â  Â  Â  Â  grammar_info = analysis[0]['analysis'][0]['gr']
Â  Â  Â  Â  Â  Â  parts = re.split(r'[,=]', grammar_info, 1)
Â  Â  Â  Â  Â  Â  pos_abbr_base = parts[0].strip()
Â  Â  Â  Â  Â  Â  pos_full = grammar_info.split(',')[0].strip()
Â  Â  Â  Â  Â  Â  if pos_full in POS_MAP:
Â  Â  Â  Â  Â  Â  Â  Â  return POS_MAP[pos_full]
Â  Â  Â  Â  Â  Â  return POS_MAP.get(pos_abbr_base, 'í’ˆì‚¬')Â 
Â  Â  return 'í’ˆì‚¬'

# ---------------------- OCR í´ë¼ì´ì–¸íŠ¸ ë° í•¨ìˆ˜ (Gemini ë””ë²„ê¹… í†µí•©) ----------------------
# ---------------------- OCR í´ë¼ì´ì–¸íŠ¸ ë° í•¨ìˆ˜ ----------------------

def get_gemini_client():
Â  Â  api_key = st.secrets.get("GEMINI_API_KEY", os.getenv("GEMINI_API_KEY"))
Â  Â  return genai.Client(api_key=api_key) if api_key else None

@st.cache_resource(show_spinner=False)
def get_vision_client():
Â  Â  client = get_gemini_client() # Gemini í´ë¼ì´ì–¸íŠ¸ ë¯¸ë¦¬ ê°€ì ¸ì˜¤ê¸°
Â  Â  # Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìœ¼ë©´ Vision API ë””ë²„ê¹… ë¡œì§ ì‚¬ìš© ë¶ˆê°€
Â  Â  if client is None:
Â  Â  Â  Â  # ì´ ê²½ìš°, Secretsê°€ ì—†ê±°ë‚˜ ì˜ëª»ë˜ì–´ë„ Geminiê°€ ë””ë²„ê¹… ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•  ìˆ˜ ì—†ìŒ
Â  Â  Â  Â  st.error("Vision API ì´ˆê¸°í™” ì „ì— Gemini API í‚¤ê°€ ì„¤ì •ë˜ì–´ì•¼ Secrets ë””ë²„ê¹… ë©”ì‹œì§€ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
Â  Â  Â  Â  return None

Â  Â  try:
Â  Â  Â  Â  # Secretsì—ì„œ JSON í‚¤ë¥¼ ë¶ˆëŸ¬ì˜´
Â  Â  Â  Â  key_json = st.secrets.get("GOOGLE_APPLICATION_CREDENTIALS_JSON")Â 
@@ -146,32 +139,16 @@
Â  Â  Â  Â  import google.auth
Â  Â  Â  Â  import google.cloud.vision

Â  Â  Â  Â  # ğŸŒŸğŸŒŸğŸŒŸ 1. JSON ìœ íš¨ì„± ê²€ì‚¬ ë° ë¡œë“œ ì‹œë„ (ì˜¤ë¥˜ í¬ì°© ì§€ì ) ğŸŒŸğŸŒŸğŸŒŸ
Â  Â  Â  Â  # ğŸŒŸğŸŒŸğŸŒŸ 1. JSON ìœ íš¨ì„± ê²€ì‚¬ ë° ë¡œë“œ ì‹œë„ (Gemini ë””ë²„ê¹… ì œê±°, ì›ë³¸ ì˜¤ë¥˜ í¬ì°©) ğŸŒŸğŸŒŸğŸŒŸ
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # ìœ ë‹ˆì½”ë“œ ì œì–´ ë¬¸ìë¥¼ ê°•ì œë¡œ ë¬´ì‹œí•˜ê³  ASCIIë¡œ í´ë¦°í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. (Invalid control character í•´ê²° ì‹œë„)
Â  Â  Â  Â  Â  Â  # ìœ ë‹ˆì½”ë“œ ì œì–´ ë¬¸ìë¥¼ ê°•ì œë¡œ ë¬´ì‹œí•˜ê³  ASCIIë¡œ í´ë¦°í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. (ìµœëŒ€í•œ ì˜¤ë¥˜ íšŒí”¼)
Â  Â  Â  Â  Â  Â  cleaned_json_string = key_json.encode('ascii', 'ignore').decode('ascii')
Â  Â  Â  Â  Â  Â  key_data = json.loads(cleaned_json_string)

Â  Â  Â  Â  except Exception as json_error:
Â  Â  Â  Â  Â  Â  # JSON ë¡œë“œ ì‹¤íŒ¨ ì‹œ, Geminiì—ê²Œ ì˜¤ë¥˜ ë¶„ì„ ìš”ì²­
Â  Â  Â  Â  Â  Â  error_details = f"Python Traceback: {str(json_error)}\n\në¬¸ì œì˜ JSON ì‹œì‘ ë¶€ë¶„: {key_json[:300]}"
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  debugging_prompt = f"""
Â  Â  Â  Â  Â  Â  ì£¼ì–´ì§„ Python Tracebackê³¼ JSON ì‹œì‘ ë¶€ë¶„ì„ ë¶„ì„í•˜ì—¬, JSON íŒŒì‹± ì˜¤ë¥˜(íŠ¹íˆ 'Invalid control character' ì˜¤ë¥˜)ê°€ ë°œìƒí•œ ì´ìœ ì™€, ì‚¬ìš©ìê°€ Secrets íŒŒì¼ì— ì–´ë–¤ ë¬¸ìë¥¼ ì˜ëª» ì…ë ¥í–ˆëŠ”ì§€ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

Â  Â  Â  Â  Â  Â  {error_details}
Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  gemini_res = client.models.generate_content(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model="gemini-2.0-flash",Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  contents=debugging_prompt
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  st.error("ğŸš¨ JSON í‚¤ íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ (Gemini ë¶„ì„ ê²°ê³¼)")
Â  Â  Â  Â  Â  Â  Â  Â  st.info(gemini_res.text.strip())
Â  Â  Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  Â  Â  st.error("ğŸš¨ JSON í‚¤ íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ. Gemini ë””ë²„ê¹…ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. Secretsì˜ ë¬¸ìì—´ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # JSON ë¡œë“œ ì‹¤íŒ¨ ì‹œ, Pythonì˜ ì›ë³¸ ì˜¤ë¥˜ë¥¼ ì¶œë ¥
Â  Â  Â  Â  Â  Â  st.error("ğŸš¨ Secrets JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ: ìœ íš¨í•˜ì§€ ì•Šì€ ë¬¸ì í¬í•¨")
Â  Â  Â  Â  Â  Â  st.code(f"Secrets Value Start:\n{key_json[:300]}...\n\nJSON Error: {str(json_error)}", language="python")
Â  Â  Â  Â  Â  Â  return None

Â  Â  Â  Â  # 2. Credential ìƒì„± ë° í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜
@@ -220,8 +197,6 @@

# ---------------------- 1. Gemini ì—°ë™ í•¨ìˆ˜ (TTL ë° JSON Schema ì ìš©) ----------------------

# (ì´í•˜ Gemini ë° UI ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.)

def get_word_info_schema(is_verb: bool):
Â  Â  """Gemini ì‘ë‹µì˜ JSON ìŠ¤í‚¤ë§ˆë¥¼ ì •ì˜í•©ë‹ˆë‹¤."""
Â  Â  schema = {
@@ -466,317 +441,318 @@
Â  Â  image_bytes = uploaded_file.getvalue()
Â  Â  ocr_result = detect_text_from_image(image_bytes)Â 

Â  Â  # OCR ê²°ê³¼ ì¶œë ¥ ë¡œì§
Â  Â  if ocr_result and not ocr_result.startswith(("OCR API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨", "Vision API ì˜¤ë¥˜", "OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")):
Â  Â  Â  Â  st.session_state.ocr_output_text = ocr_result
Â  Â  Â  Â  st.session_state.input_text_area = ocr_result
Â  Â  Â  Â  st.session_state.translated_text = ""
Â  Â  Â  Â  st.success("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
Â  Â  else:
Â  Â  Â  Â  st.error(ocr_result)

# í…ìŠ¤íŠ¸ ë°˜ì˜ ë²„íŠ¼ ì¶”ê°€
st.button(
Â  Â  "ì¤‘ê¸‰ëŸ¬ì‹œì•„ì–´ì—°ìŠµ í…ìŠ¤íŠ¸ ë°˜ì˜í•˜ê¸°(êµì¬ 2ê¶Œ 44í˜ì´ì§€)",
Â  Â  on_click=load_default_text,
Â  Â  help="êµì¬ ì—°ìŠµìš© í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ì°½ì— ë°˜ì˜í•©ë‹ˆë‹¤."
)

st.subheader("ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸")
current_text = st.text_area(
Â  Â  "ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ìœ„ì— ì—…ë¡œë“œëœ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”",
Â  Â  value=st.session_state.input_text_area,
Â  Â  height=150,
Â  Â  key="input_text_area"
)


# í…ìŠ¤íŠ¸ê°€ ìˆ˜ì •ë˜ë©´ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ë²ˆì—­/ë¶„ì„ ìƒíƒœ ì´ˆê¸°í™”
if current_text != st.session_state.last_processed_text:
Â  Â  st.session_state.translated_text = ""
Â  Â  st.session_state.selected_words = []
Â  Â  st.session_state.clicked_word = None
Â  Â  st.session_state.word_info = {}
Â  Â  st.session_state.current_search_query = ""


# --- 6.2. ë‹¨ì–´ ê²€ìƒ‰ì°½ ë° ë¡œì§ ---
st.divider()
st.subheader("ë‹¨ì–´/êµ¬ ê²€ìƒ‰")
manual_input = st.text_input("ë‹¨ì–´ ë˜ëŠ” êµ¬ë¥¼ ì…ë ¥í•˜ê³  Enter (ì˜ˆ: 'Ğ¸Ğ´Ñ‘Ñ‚ Ğ¿Ğ¾ ÑƒĞ»Ğ¸Ñ†Ğµ')", key="current_search_query")

if manual_input and manual_input != st.session_state.get("last_processed_query"):
Â  Â  if manual_input not in st.session_state.selected_words:
Â  Â  Â  Â  st.session_state.selected_words.append(manual_input)

Â  Â  st.session_state.clicked_word = manual_input

Â  Â  with st.spinner(f"'{manual_input}'ì— ëŒ€í•œ ì •ë³´ ë¶„ì„ ì¤‘..."):
Â  Â  Â  Â  clean_input = manual_input
Â  Â  Â  Â  lemma = lemmatize_ru(clean_input)
Â  Â  Â  Â  pos = get_pos_ru(clean_input)
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  info = fetch_from_gemini(clean_input, lemma, pos)

Â  Â  Â  Â  Â  Â  # ê¸°ë³¸í˜•(lemma) ê¸°ì¤€ìœ¼ë¡œ ì •ë³´ ì €ì¥. ë‹¨, í˜„ì¬ ê²€ìƒ‰ì–´(token)ê°€ ë‹¤ë¥´ë©´ ì—…ë°ì´íŠ¸
Â  Â  Â  Â  Â  Â  if lemma not in st.session_state.word_info or st.session_state.word_info.get(lemma, {}).get('loaded_token') != clean_input:
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.word_info[lemma] = {**info, "loaded_token": clean_input, "pos": pos}
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  st.error(f"Gemini ì˜¤ë¥˜: {e}")

Â  Â  st.session_state.last_processed_query = manual_input

st.markdown("---")


# ---------------------- 7. í…ìŠ¤íŠ¸ í•˜ì´ë¼ì´íŒ… ë° ìƒì„¸ ì •ë³´ ë ˆì´ì•„ì›ƒ ----------------------

left, right = st.columns([2, 1])


with left:
Â  Â  st.subheader("ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ ì›ë¬¸")

Â  Â  # --- TTS ë²„íŠ¼ ë° ê°•ì„¸ ë§í¬ ---
Â  Â  col_tts, col_accent = st.columns([1, 2])

Â  Â  with col_tts:
Â  Â  Â  Â  ELEVENLABS_URL = "https://elevenlabs.io/"
Â  Â  Â  Â  st.markdown(
Â  Â  Â  Â  Â  Â  f"[â–¶ï¸ í…ìŠ¤íŠ¸ ìŒì„± ë“£ê¸° (ElevenLabs)]({ELEVENLABS_URL})",
Â  Â  Â  Â  Â  Â  unsafe_allow_html=False
Â  Â  Â  Â  )

Â  Â  with col_accent:
Â  Â  Â  Â  ACCENT_ONLINE_URL = "https://russiangram.com/"

Â  Â  Â  Â  st.markdown(
Â  Â  Â  Â  Â  Â  f"ğŸ”Š [ê°•ì„¸ í‘œì‹œ ì‚¬ì´íŠ¸ë¡œ ì´ë™ (russiangram.com)]({ACCENT_ONLINE_URL})",
Â  Â  Â  Â  Â  Â  unsafe_allow_html=False
Â  Â  Â  Â  )
Â  Â  Â  Â  st.info("â¬†ï¸ ìŒì„± ë“£ê¸° ë° ê°•ì„¸ í™•ì¸ì„ ìœ„í•´ ì™¸ë¶€ ì‚¬ì´íŠ¸ ë§í¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìƒˆ íƒ­ìœ¼ë¡œ ì—´ë¦½ë‹ˆë‹¤.")


Â  Â  # ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ í•˜ì´ë¼ì´íŒ… ì¶œë ¥ (current_text ì‚¬ìš©)
Â  Â  ru_html = get_highlighted_html(current_text, st.session_state.selected_words)
Â  Â  st.markdown(ru_html, unsafe_allow_html=True)

Â  Â  st.markdown("---")


Â  Â  # ì´ˆê¸°í™” ë²„íŠ¼
Â  Â  def reset_all_state():
Â  Â  Â  Â  st.session_state.selected_words = []
Â  Â  Â  Â  st.session_state.clicked_word = None
Â  Â  Â  Â  st.session_state.word_info = {}
Â  Â  Â  Â  st.session_state.current_search_query = ""
Â  Â  Â  Â  st.session_state.input_text_area = DEFAULT_TEST_TEXT
Â  Â  Â  Â  st.session_state.ocr_output_text = ""
Â  Â  Â  Â  st.session_state.translated_text = ""
Â  Â  Â  Â  st.session_state.last_processed_text = ""


Â  Â  st.button("ì„ íƒ ë° ê²€ìƒ‰ ì´ˆê¸°í™”", key="reset_button", on_click=reset_all_state)


# ---------------------- 7.2. ë‹¨ì–´ ìƒì„¸ ì •ë³´ (right ì»¬ëŸ¼) ----------------------
with right:
Â  Â  st.subheader("ë‹¨ì–´ ìƒì„¸ ì •ë³´")

Â  Â  current_token = st.session_state.clicked_word

Â  Â  if current_token:
Â  Â  Â  Â  clean_token = current_token
Â  Â  Â  Â  lemma = lemmatize_ru(clean_token)
Â  Â  Â  Â  info = st.session_state.word_info.get(lemma, {})

Â  Â  Â  Â  if info and "ko_meanings" in info:
Â  Â  Â  Â  Â  Â  pos = info.get("pos", "í’ˆì‚¬")
Â  Â  Â  Â  Â  Â  aspect_pair = info.get("aspect_pair")

Â  Â  Â  Â  Â  Â  # --- 1. êµ¬ ì „ì²´ì˜ ì •ë³´ í‘œì‹œ ---
Â  Â  Â  Â  Â  Â  st.markdown(f"### **{clean_token}**")

Â  Â  Â  Â  Â  Â  if pos == 'ë™ì‚¬' and aspect_pair:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**ê¸°ë³¸í˜• (ë¶ˆì™„ë£Œìƒ):** *{aspect_pair.get('imp', lemma)}*")
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**ì™„ë£Œìƒ:** *{aspect_pair.get('perf', 'ì •ë³´ ì—†ìŒ')}*")
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**í’ˆì‚¬:** {pos}")
Â  Â  Â  Â  Â  Â  elif pos == 'êµ¬ í˜•íƒœ':Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**êµ¬(å¥) í˜•íƒœ:** *{lemma}*")
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**í’ˆì‚¬:** {pos} (ê°œë³„ ë‹¨ì–´ ë¶„ì„ì„ ì°¸ê³ í•˜ì„¸ìš”)")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**ê¸°ë³¸í˜• (Lemma):** *{lemma}* ({pos})")

Â  Â  Â  Â  Â  Â  st.divider()

Â  Â  Â  Â  Â  Â  ko_meanings = info.get("ko_meanings", [])
Â  Â  Â  Â  Â  Â  examples = info.get("examples", [])

Â  Â  Â  Â  Â  Â  if ko_meanings:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("#### í•œêµ­ì–´ ëœ»")
Â  Â  Â  Â  Â  Â  Â  Â  for m in ko_meanings:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"- **{m}**")

Â  Â  Â  Â  Â  Â  if examples:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("#### ğŸ“– ì˜ˆë¬¸")
Â  Â  Â  Â  Â  Â  Â  Â  for ex in examples:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"- {ex.get('ru', '')}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"â€ƒâ†’ {ex.get('ko', '')}")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  if ko_meanings and ko_meanings[0].startswith("'{current_token}'ì˜ API í‚¤ ì—†ìŒ"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì˜ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
Â  Â  Â  Â  Â  Â  Â  Â  elif ko_meanings and ko_meanings[0].startswith(("API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì˜¤ë¥˜", "API í˜¸ì¶œ ë˜ëŠ” JSON íŒŒì‹± ì˜¤ë¥˜")):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Gemini API ì˜¤ë¥˜: {ko_meanings[0]}")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("ì˜ˆë¬¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

Â  Â  Â  Â  Â  Â  # --- 2. êµ¬ ì•ˆì— ìˆëŠ” ê°œë³„ ë‹¨ì–´ ì •ë³´ í‘œì‹œ (ìš”ì²­ ì‚¬í•­ ë°˜ì˜: ê°„ëµ ëœ» ë¡œë“œ) ---
Â  Â  Â  Â  Â  Â  if pos == 'êµ¬ í˜•íƒœ':
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("#### ë‚±ë§(í† í°) ë¶„ì„")

Â  Â  Â  Â  Â  Â  Â  Â  individual_words = clean_token.split()Â 

Â  Â  Â  Â  Â  Â  Â  Â  for word in individual_words:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  processed_word = re.sub(r'[.,!?;:"]', '', word)Â 

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not processed_word:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  token_lemma = lemmatize_ru(processed_word)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  token_pos = get_pos_ru(processed_word)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  token_info = st.session_state.word_info.get(token_lemma)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not token_info or token_info.get('pos') == 'êµ¬ í˜•íƒœ':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  loaded_info = fetch_from_gemini(token_lemma, token_lemma, token_pos)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if loaded_info.get("ko_meanings") and not loaded_info["ko_meanings"][0].startswith(("API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì˜¤ë¥˜", "API í˜¸ì¶œ ë˜ëŠ” JSON íŒŒì‹± ì˜¤ë¥˜")):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.word_info[token_lemma] = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  **loaded_info,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "loaded_token": token_lemma,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "pos": token_pos
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  token_info = st.session_state.word_info[token_lemma]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**{word}** (`{token_lemma}`) â†’ ëœ» ì •ë³´ ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ì˜¤ë¥˜")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**{word}** (`{token_lemma}`) â†’ API í˜¸ì¶œ ì˜¤ë¥˜")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if token_info:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  token_pos = token_info.get("pos", "í’ˆì‚¬")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  token_meanings = token_info.get("ko_meanings", [])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_meaning = "; ".join(token_meanings[:1])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**{word}** (`{token_lemma}` - {token_pos}) â†’ **{display_meaning}**")

Â  Â  Â  Â  Â  Â  # --- 3. ì™¸ë¶€ ê²€ìƒ‰ ë§í¬ ---
Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  encoded_query = urllib.parse.quote(clean_token)

Â  Â  Â  Â  Â  Â  multitran_url = f"https://www.multitran.com/m.exe?s={encoded_query}&l1=1&l2=2"
Â  Â  Â  Â  Â  Â  corpus_url = f"http://search.ruscorpora.ru/search.xml?text={encoded_query}&env=alpha&mode=main&sort=gr_tagging&lang=ru&nodia=1"

Â  Â  Â  Â  Â  Â  st.markdown("#### ğŸŒ ì™¸ë¶€ ê²€ìƒ‰")
Â  Â  Â  Â  Â  Â  col1, col2 = st.columns(2)

Â  Â  Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"[Multitran ê²€ìƒ‰]({multitran_url})")

Â  Â  Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"[êµ­ë¦½ ì½”í¼ìŠ¤ ê²€ìƒ‰]({corpus_url})")

Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.warning("ë‹¨ì–´ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì´ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

Â  Â  else:
Â  Â  Â  Â  st.info("ê²€ìƒ‰ì°½ì— ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ë©´ ì—¬ê¸°ì— ìƒì„¸ ì •ë³´ê°€ í‘œì‹œë©ë‹ˆë‹¤.")


# ---------------------- 8. í•˜ë‹¨: ëˆ„ì  ëª©ë¡ + CSV ----------------------
st.divider()
st.subheader("ë‹¨ì–´ ëª©ë¡ (ê¸°ë³¸í˜• ê¸°ì¤€)")

selected = st.session_state.selected_words
word_info = st.session_state.word_info

if word_info:
Â  Â  rows = []
Â  Â  processed_lemmas = set()

Â  Â  for tok in selected:
Â  Â  Â  Â  clean_tok = tok
Â  Â  Â  Â  lemma = lemmatize_ru(clean_tok)
Â  Â  Â  Â  if lemma not in processed_lemmas and lemma in word_info:
Â  Â  Â  Â  Â  Â  info = word_info[lemma]
Â  Â  Â  Â  Â  Â  if info.get("ko_meanings") and not info["ko_meanings"][0].startswith(("API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì˜¤ë¥˜", "API í˜¸ì¶œ ë˜ëŠ” JSON íŒŒì‹± ì˜¤ë¥˜")):
Â  Â  Â  Â  Â  Â  Â  Â  pos = info.get("pos", "í’ˆì‚¬")

Â  Â  Â  Â  Â  Â  Â  Â  if pos == 'ë™ì‚¬' and info.get("aspect_pair"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  imp = info['aspect_pair'].get('imp', lemma)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  perf = info['aspect_pair'].get('perf', 'ì •ë³´ ì—†ìŒ')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  base_form = f"{imp} / {perf}"
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  base_form = lemma

Â  Â  Â  Â  Â  Â  Â  Â  short = "; ".join(info["ko_meanings"][:2])
Â  Â  Â  Â  Â  Â  Â  Â  short = f"({pos}) {short}"

Â  Â  Â  Â  Â  Â  Â  Â  rows.append({"ê¸°ë³¸í˜•": base_form, "ëŒ€í‘œ ëœ»": short})
Â  Â  Â  Â  Â  Â  Â  Â  processed_lemmas.add(lemma)

Â  Â  if rows:
Â  Â  Â  Â  df = pd.DataFrame(rows)
Â  Â  Â  Â  st.dataframe(df, hide_index=True)
Â  Â  else:
Â  Â  Â  Â  st.info("ì„ íƒëœ ë‹¨ì–´ì˜ ì •ë³´ê°€ ë¡œë“œ ì¤‘ì´ê±°ë‚˜, í‘œì‹œí•  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")


# ---------------------- 9. í•˜ë‹¨: í•œêµ­ì–´ ë²ˆì—­ë³¸ ----------------------
st.divider()
st.subheader("í•œêµ­ì–´ ë²ˆì—­ë³¸")

if st.session_state.translated_text == "" or current_text != st.session_state.last_processed_text:
Â  Â  st.session_state.translated_text = translate_text(
Â  Â  Â  Â  current_text,
Â  Â  Â  Â  st.session_state.selected_words
Â  Â  )
Â  Â  st.session_state.last_processed_text = current_text

translated_text = st.session_state.translated_text

if translated_text.startswith("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€"):
Â  Â  st.error(translated_text)
elif translated_text.startswith("ë²ˆì—­ ì˜¤ë¥˜ ë°œìƒ"):
Â  Â  st.error(translated_text)
else:
Â  Â  st.markdown(f'<div class="text-container" style="color: #333; font-weight: 500;">{translated_text}</div>', unsafe_allow_html=True)


# ---------------------- 10. í™ë³´ ì˜ìƒ ì‚½ì… (í˜ì´ì§€ ë§¨ ì•„ë˜ë¡œ ì´ë™) ----------------------

st.divider()

_, col_video = st.columns([1, 1])

with col_video:
Â  Â  st.subheader("ğŸ¬ í”„ë¡œì íŠ¸ í™ë³´ ì˜ìƒ")
Â  Â  if YOUTUBE_VIDEO_ID:
Â  Â  Â  Â  video_html = youtube_embed_html(YOUTUBE_VIDEO_ID)Â 
Â  Â  Â  Â  st.markdown(video_html, unsafe_allow_html=True)
Â  Â  Â  Â  st.caption(f"YouTube ì˜ìƒ ID: {YOUTUBE_VIDEO_ID}")Â 
Â  Â  else:
Â  Â  Â  Â  st.warning("í™ë³´ ì˜ìƒì„ í‘œì‹œí•˜ë ¤ë©´ YOUTUBE_VIDEO_IDë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")


# ---------------------- 11. ì €ì‘ê¶Œ í‘œì‹œ (í˜ì´ì§€ ìµœí•˜ë‹¨) ----------------------
st.markdown("---")
st.markdown("""
<div style="text-align: center; font-size: 0.75em; color: #888;">
Â  Â  ì´ í˜ì´ì§€ëŠ” ì—°ì„¸ëŒ€í•™êµ ë…¸ì–´ë…¸ë¬¸í•™ê³¼ 25-2 ëŸ¬ì‹œì•„ì–´ êµìœ¡ë¡  5íŒ€ì˜ í”„ë¡œì íŠ¸ ê²°ê³¼ë¬¼ì…ë‹ˆë‹¤.
Â  Â  <br>
Â  Â  ë³¸ í˜ì´ì§€ì˜ ë‚´ìš©, ê¸°ëŠ¥ ë° ë°ì´í„°ë¥¼ í•™ìŠµ ëª©ì  ì´ì™¸ì˜ ìš©ë„ë¡œ ë¬´ë‹¨ ë³µì œ, ë°°í¬, ìƒì—…ì  ì´ìš©í•  ê²½ìš°,
Â  Â  ê´€ë ¨ ë²•ë ¹ì— ë”°ë¼ ë¯¼ì‚¬ìƒ ì†í•´ë°°ìƒ ì²­êµ¬ ë° í˜•ì‚¬ìƒ ì²˜ë²Œì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
</div>
""", unsafe_allow_html=True)
