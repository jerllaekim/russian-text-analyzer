import streamlit as st
import re
import os
import json
import pandas as pd
from pymystem3 import Mystem
from google import genai
from google.cloud import vision
import io
import urllib.parse
from typing import Union

# ---------------------- 0. ì´ˆê¸° ì„¤ì • ë° ì„¸ì…˜ ìƒíƒœ ----------------------

NEW_DEFAULT_TEXT = """Ğ¢Ğ¾Ğ¼ Ğ¶Ğ¸Ğ²Ñ‘Ñ‚ Ğ² Ğ¡Ğ°Ğ½ĞºÑ‚-ĞŸĞµÑ‚ĞµÑ€Ğ±ÑƒÑ€Ğ³Ğµ ÑƒĞ¶Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼ĞµÑÑÑ†ĞµĞ². Ğ’ ÑÑƒĞ±Ğ±Ğ¾Ñ‚Ñƒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ±Ñ‹Ğ»Ğ° Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ°Ñ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ°, Ğ¢Ğ¾Ğ¼ Ñ€ĞµÑˆĞ¸Ğ» Ğ¿Ğ¾Ğ¹Ñ‚Ğ¸ Ğ² Ğ˜ÑĞ°Ğ°ĞºĞ¸ĞµĞ²ÑĞºĞ¸Ğ¹ ÑĞ¾Ğ±Ğ¾Ñ€. Ğ¢Ğ¾Ğ¼ Ğ´Ğ°Ğ²Ğ½Ğ¾ Ğ¼ĞµÑ‡Ñ‚Ğ°Ğ» Ğ¿Ğ¾Ğ±Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ² ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ğ±Ğ¾Ñ€Ğµ. Ğ˜ÑĞ°Ğ°ĞºĞ¸ĞµĞ²ÑĞºĞ¸Ğ¹ ÑĞ¾Ğ±Ğ¾Ñ€ â€” Ğ¾Ğ´Ğ½Ğ¾ Ğ¸Ğ· ÑĞ°Ğ¼Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¡Ğ°Ğ½ĞºÑ‚-ĞŸĞµÑ‚ĞµÑ€Ğ±ÑƒÑ€Ğ³Ğµ, ĞµĞ³Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ²Ğ¸Ğ´ĞµÑ‚ÑŒ
Ğ´Ğ°Ğ¶Ğµ Ğ¸Ğ·Ğ´Ğ°Ğ»ĞµĞºĞ°. ĞšĞ¾Ğ³Ğ´Ğ° Ğ¢Ğ¾Ğ¼ Ğ³ÑƒĞ»ÑĞ» Ğ¿Ğ¾ Ñ†ĞµĞ½Ñ‚Ñ€Ñƒ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°, Ğ¾Ğ½ Ğ¾Ñ‚Ğ¾Ğ²ÑÑĞ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ» Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ ĞºÑƒĞ¿Ğ¾Ğ» ÑĞ¾Ğ±Ğ¾Ñ€Ğ°. Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¢Ğ¾Ğ¼ Ñ€ĞµÑˆĞ¸Ğ» Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ ÑĞ¾Ğ±Ğ¾Ñ€ ÑĞ½Ğ°Ñ€ÑƒĞ¶Ğ¸. ĞĞ½ Ğ¿Ñ€Ğ¸ÑˆÑ‘Ğ» Ğ½Ğ° Ğ˜ÑĞ°Ğ°ĞºĞ¸ĞµĞ²ÑĞºÑƒÑ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ â€” Ğ¾Ñ‚ÑÑĞ´Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞºÑ€Ğ°ÑĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ Ğ½Ğ° ÑĞ¾Ğ±Ğ¾Ñ€. ĞŸĞ¾Ñ‚Ğ¾Ğ¼ Ğ¢Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ¾ÑˆÑ‘Ğ» Ğº ÑĞ¾Ğ±Ğ¾Ñ€Ñƒ Ğ¿Ğ¾Ğ±Ğ»Ğ¸Ğ¶Ğµ, Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ» ĞµĞ³Ğ¾ ÑĞ¿ĞµÑ€ĞµĞ´Ğ¸, ÑĞ·Ğ°Ğ´Ğ¸, 2 Ñ€Ğ°Ğ·Ğ° Ğ¾Ğ±Ğ¾ÑˆÑ‘Ğ» Ğ²Ğ¾ĞºÑ€ÑƒĞ³ ÑĞ¾Ğ±Ğ¾Ñ€Ğ°, Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼ Ğ²Ğ¾ÑˆÑ‘Ğ» Ğ²Ğ½ÑƒÑ‚Ñ€ÑŒ. Ğ’Ğ½ÑƒÑ‚Ñ€Ğ¸ ÑĞ¾Ğ±Ğ¾Ñ€ Ğ¾Ñ‡ĞµĞ½ÑŒ ĞºÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğ¹. Ğ¢Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ», Ñ‡Ñ‚Ğ¾ ĞºÑƒĞ¿Ğ¾Ğ» ÑĞ¾Ğ±Ğ¾Ñ€Ğ° â€” Ñ‚Ñ€ĞµÑ‚Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ğµ Ğ² Ğ•Ğ²Ñ€Ğ¾Ğ¿Ğµ. Ğ¢Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ½ÑĞ» Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñƒ Ğ²Ğ²ĞµÑ€Ñ… Ğ¸ ÑƒĞ²Ğ¸Ğ´ĞµĞ», Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´ ĞºÑƒĞ¿Ğ¾Ğ»Ğ¾Ğ¼ Â«Ğ»ĞµÑ‚Ğ°ĞµÑ‚Â» ÑĞµÑ€ĞµĞ±Ñ€ÑĞ½Ñ‹Ğ¹ Ğ³Ğ¾Ğ»ÑƒĞ±ÑŒ. Ğ¢Ğ¾Ğ¼ Ğ¿Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ» Ğ²Ğ¾ĞºÑ€ÑƒĞ³: Ğ²Ğ¿ĞµÑ€ĞµĞ´Ğ¸, ÑĞ·Ğ°Ğ´Ğ¸, ÑĞ¿Ñ€Ğ°Ğ²Ğ°, ÑĞ»ĞµĞ²Ğ° â€” Ğ²ĞµĞ·Ğ´Ğµ Ğ±Ñ‹Ğ»Ğ¸ ĞºÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğµ Ğ¸ĞºĞ¾Ğ½Ñ‹.
ĞŸĞ¾Ñ‚Ğ¾Ğ¼ Ğ¢Ğ¾Ğ¼ Ñ€ĞµÑˆĞ¸Ğ» Ğ¿Ğ¾Ğ´Ğ½ÑÑ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ»Ğ¾Ğ½Ğ½Ğ°Ğ´Ñƒ ÑĞ¾Ğ±Ğ¾Ñ€Ğ°. Ğ’ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ´ĞµĞ½ÑŒ Ğ² ÑĞ¾Ğ±Ğ¾Ñ€Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ÑƒÑ€Ğ¸ÑÑ‚Ğ¾Ğ²: Ğ¾Ğ´Ğ½Ğ¸ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑŒ Ğ²Ğ²ĞµÑ€Ñ…, Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¿ÑƒÑĞºĞ°Ğ»Ğ¸ÑÑŒ Ğ²Ğ½Ğ¸Ğ·. Ğ¢Ğ¾Ğ¼ Ñ‚Ğ¾Ğ¶Ğµ Ğ¿Ğ¾Ğ´Ğ½ÑĞ»ÑÑ Ğ²Ğ²ĞµÑ€Ñ…. ĞÑ‚Ñ‚ÑƒĞ´Ğ°, ÑĞ²ĞµÑ€Ñ…Ñƒ, Ñ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ 43 (ÑĞ¾Ñ€Ğ¾ĞºĞ° Ñ‚Ñ€Ñ‘Ñ…) Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞºÑ€Ğ°ÑĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ Ğ½Ğ° Ñ†ĞµĞ½Ñ‚Ñ€ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°. Ğ¢Ğ¾Ğ¼ ÑƒĞ²Ğ¸Ğ´ĞµĞ» Ğ”Ğ²Ğ¾Ñ€Ñ†Ğ¾Ğ²ÑƒÑ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ, ĞŸĞµÑ‚Ñ€Ğ¾Ğ¿Ğ°Ğ²Ğ»Ğ¾Ğ²ÑĞºÑƒÑ ĞºÑ€ĞµĞ¿Ğ¾ÑÑ‚ÑŒ, ĞºÑ€Ñ‹ÑˆĞ¸ Ğ´Ğ¾Ğ¼Ğ¾Ğ², Ğ° Ğ½Ğ°Ğ´ ĞºÑ€Ñ‹ÑˆĞ°Ğ¼Ğ¸ Ğ»ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ‚Ğ¸Ñ†Ñ‹.
Ğ¢Ğ¾Ğ¼Ñƒ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°ÑÑŒ ÑĞºÑĞºÑƒÑ€ÑĞ¸Ñ. ĞĞ½ Ğ¿Ğ¾ÑĞ¾Ğ²ĞµÑ‚Ğ¾Ğ²Ğ°Ğ» Ğ´Ñ€ÑƒĞ·ÑŒÑĞ¼ Ğ¿Ğ¾ÑĞµÑ‚Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ±Ğ¾Ñ€ Ğ¸ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ½ÑÑ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ»Ğ¾Ğ½Ğ½Ğ°Ğ´Ñƒ."""

DEFAULT_TEST_TEXT = "Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¸Ğ´Ñ‘Ñ‚ Ğ¿Ğ¾ ÑƒĞ»Ğ¸Ñ†Ğµ. Ğ­Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°. Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾. Ğ¯ Ñ‡Ğ°ÑÑ‚Ğ¾ Ñ‡Ğ¸Ñ‚Ğ°Ñ ÑÑ‚Ñƒ ĞºĞ½Ğ¸Ğ³Ñƒ."


# ---------------------- 0.1. í˜ì´ì§€ ì„¤ì • ë° ë°°ë„ˆ ì‚½ì… ----------------------
st.set_page_config(page_title="ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ê¸°", layout="wide")

# ğŸŒŸ ë°°ë„ˆ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
IMAGE_FILE_PATH = "banner.png"

try:
    st.image(IMAGE_FILE_PATH, use_column_width=True)
except FileNotFoundError:
    st.warning(f"ë°°ë„ˆ ì´ë¯¸ì§€ íŒŒì¼ ({IMAGE_FILE_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. GitHub ì €ì¥ì†Œì— ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  íŒŒì¼ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.markdown("###")

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "selected_words" not in st.session_state:
    st.session_state.selected_words = []
if "clicked_word" not in st.session_state:
    st.session_state.clicked_word = None
if "word_info" not in st.session_state:
    st.session_state.word_info = {}
if "current_search_query" not in st.session_state:
    st.session_state.current_search_query = ""
if "ocr_output_text" not in st.session_state:
    st.session_state.ocr_output_text = ""
if "input_text_area" not in st.session_state:
    st.session_state.input_text_area = DEFAULT_TEST_TEXT
if "translated_text" not in st.session_state:
    st.session_state.translated_text = ""
if "last_processed_text" not in st.session_state:
    st.session_state.last_processed_text = ""
if "last_processed_query" not in st.session_state:
    st.session_state.last_processed_query = ""


mystem = Mystem()

# ---------------------- 0.2. YouTube ì„ë² ë“œ í•¨ìˆ˜ ë° ID ì •ì˜ ----------------------

# ğŸ“Œ ğŸš¨ ì¤‘ìš”: ì—¬ê¸°ì— í™ë³´ ì˜ìƒì˜ YouTube IDë¥¼ ë„£ì–´ì£¼ì„¸ìš”. (ë¬¸ìì—´ë¡œ, ë”°ì˜´í‘œ ì•ˆì—)
YOUTUBE_VIDEO_ID = "wJ65i_gDfT0" 

def youtube_embed_html(video_id: str):
    """ì§€ì •ëœ YouTube IDë¡œ ë°˜ì‘í˜• ì„ë² ë“œ HTMLì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    embed_url = f"https://www.youtube.com/embed/{video_id}?autoplay=0&rel=0"
    
    html_code = f"""
    <div class="video-container-wrapper">
        <div class="video-responsive">
            <iframe
                src="{embed_url}"
                frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen
                title="í”„ë¡œì íŠ¸ í™ë³´ ì˜ìƒ"
            ></iframe>
        </div>
    </div>
    """
    return html_code


# ---------------------- í’ˆì‚¬ ë³€í™˜ ë”•ì…”ë„ˆë¦¬ ë° Mystem í•¨ìˆ˜ ----------------------
POS_MAP = {
    'S': 'ëª…ì‚¬', 'V': 'ë™ì‚¬', 'A': 'í˜•ìš©ì‚¬', 'ADV': 'ë¶€ì‚¬', 'PR': 'ì „ì¹˜ì‚¬',
    'CONJ': 'ì ‘ì†ì‚¬', 'INTJ': 'ê°íƒ„ì‚¬', 'PART': 'ë¶ˆë³€í™”ì‚¬', 'NUM': 'ìˆ˜ì‚¬',
    'APRO': 'ëŒ€ëª…ì‚¬ì  í˜•ìš©ì‚¬', 'ANUM': 'ì„œìˆ˜ì‚¬', 'SPRO': 'ëŒ€ëª…ì‚¬',
    'PRICL': 'ë™ì‚¬ë¶€ì‚¬',
    'COMP': 'ë¹„êµê¸‰', 'A=cmp': 'ë¹„êµê¸‰ í˜•ìš©ì‚¬', 'ADV=cmp': 'ë¹„êµê¸‰ ë¶€ì‚¬',
    'ADVB': 'ë¶€ì‚¬',
}

@st.cache_data(show_spinner=False)
def lemmatize_ru(word: str) -> str:
    if ' ' in word.strip():
        return word.strip()
    if re.fullmatch(r'\w+', word, flags=re.UNICODE):
        lemmas = mystem.lemmatize(word)
        return (lemmas[0] if lemmas else word).strip()
    return word

@st.cache_data(show_spinner=False)
def get_pos_ru(word: str) -> str:
    # ê³µë°±ì´ í¬í•¨ëœ ê²½ìš° 'êµ¬ í˜•íƒœ'ë¡œ ë°˜í™˜ (êµ¬ ë¶„ì„ ê¸°ëŠ¥ì„ ìœ„í•¨)
    if ' ' in word.strip():
        return 'êµ¬ í˜•íƒœ' 
    if re.fullmatch(r'\w+', word, flags=re.UNICODE):
        analysis = mystem.analyze(word)
        if analysis and 'analysis' in analysis[0] and analysis[0]['analysis']:
            grammar_info = analysis[0]['analysis'][0]['gr']
            
            parts = re.split(r'[,=]', grammar_info, 1)
            pos_abbr_base = parts[0].strip()

            pos_full = grammar_info.split(',')[0].strip()

            if pos_full in POS_MAP:
                return POS_MAP[pos_full]
            
            return POS_MAP.get(pos_abbr_base, 'í’ˆì‚¬')
            
    return 'í’ˆì‚¬'

# ---------------------- OCR í•¨ìˆ˜ ----------------------
@st.cache_data(show_spinner="ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘")
def detect_text_from_image(image_bytes):
    try:
        if st.secrets.get("GCP_SA_KEY"):
            with open("temp_sa_key.json", "w") as f:
                json.dump(st.secrets["GCP_SA_KEY"], f)
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "temp_sa_key.json"
        elif "GOOGLE_APPLICATION_CREDENTIALS" not in os.environ:
            return "OCR API í‚¤(GOOGLE_APPLICATION_CREDENTIALS)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Cloud Vision API ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

        client = vision.ImageAnnotatorClient()
        image = vision.Image(content=image_bytes)
        response = client.text_detection(image=image)
        texts = response.text
            
        if response.error.message:
            return f"Vision API ì˜¤ë¥˜: {response.error.message}"
            
        return texts.split('\n', 1)[0] if texts else "ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    except Exception as e:
        return f"OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


# ---------------------- 1. Gemini ì—°ë™ í•¨ìˆ˜ ----------------------

def get_gemini_client():
    api_key = st.secrets.get("GEMINI_API_KEY", os.getenv("GEMINI_API_KEY"))
    return genai.Client(api_key=api_key) if api_key else None

@st.cache_data(show_spinner=False)
def fetch_from_gemini(word, lemma, pos):
    client = get_gemini_client()
    if not client:
        return {"ko_meanings": [f"'{word}'ì˜ API í‚¤ ì—†ìŒ (GEMINI_API_KEY ì„¤ì • í•„ìš”)"], "examples": []}
    
    SYSTEM_PROMPT = "ë„ˆëŠ” ëŸ¬ì‹œì•„ì–´-í•œêµ­ì–´ í•™ìŠµ ë„ìš°ë¯¸ì´ë‹¤. ëŸ¬ì‹œì•„ì–´ ë‹¨ì–´ì— ëŒ€í•´ ê°„ë‹¨í•œ í•œêµ­ì–´ ëœ»ê³¼ ì˜ˆë¬¸ì„ ìµœëŒ€ ë‘ ê°œë§Œ ì œê³µí•œë‹¤. í•œêµ­ì–´ ëœ»ì„ ì œê³µí•  ë•Œ ê²© ì •ë³´, ë¬¸ë²• ì •ë³´ ë“± ë¶ˆí•„ìš”í•œ ë¶€ê°€ ì •ë³´ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤. ë§Œì•½ ë™ì‚¬(V)ì´ë©´, ë¶ˆì™„ë£Œìƒ(imp)ê³¼ ì™„ë£Œìƒ(perf) í˜•íƒœë¥¼ í•¨ê»˜ ì œê³µí•´ì•¼ í•œë‹¤. ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•œë‹¤."
    
    if pos == 'ë™ì‚¬':
        prompt = f"""{SYSTEM_PROMPT}
ë‹¨ì–´: {word}
ê¸°ë³¸í˜•: {lemma}
{{ "ko_meanings": ["ëœ»1", "ëœ»2"], "aspect_pair": {{"imp": "ë¶ˆì™„ë£Œìƒ ë™ì‚¬", "perf": "ì™„ë£Œìƒ ë™ì‚¬"}}, "examples": [ {{"ru": "ì˜ˆë¬¸1", "ko": "ë²ˆì—­1"}}, {{"ru": "ì˜ˆë¬¸2", "ko": "ë²ˆì—­2"}} ] }}
"""
    else:
        prompt = f"""{SYSTEM_PROMPT}
ë‹¨ì–´: {word}
ê¸°ë³¸í˜•: {lemma}
{{ "ko_meanings": ["ëœ»1", "ëœ»2"], "examples": [ {{"ru": "ì˜ˆë¬¸1", "ko": "ë²ˆì—­1"}}, {{"ru": "ì˜ˆë¬¸2", "ko": "ë²ˆì—­2"}} ] }}
"""
    
    res = client.models.generate_content(model="gemini-2.0-flash", contents=prompt)
    text = res.text.strip()
    
    try:
        # JSON íŒŒì‹± ë¡œì§
        if text.startswith("```"):
            text = text.strip("`")
            lines = text.splitlines()
            if lines and lines[0].lower().startswith("json"):
                text = "\n".join(lines[1:])
            elif lines:
                text = "\n".join(lines)
                
        start_index = text.find('{')
        end_index = text.rfind('}')
        
        if start_index != -1 and end_index != -1 and end_index > start_index:
            json_text = text[start_index : end_index + 1]
        else:
            json_text = text
            
        data = json.loads(json_text)
        
        if 'examples' in data and len(data['examples']) > 2:
            data['examples'] = data['examples'][:2]
        return data
        
    except json.JSONDecodeError:
        return {"ko_meanings": ["JSON íŒŒì‹± ì˜¤ë¥˜"], "examples": []}


# ---------------------- 2. í…ìŠ¤íŠ¸ ë²ˆì—­ í•¨ìˆ˜ ----------------------

@st.cache_data(show_spinner="í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ì¤‘...")
def translate_text(russian_text, highlight_words):
    client = get_gemini_client()
    if not client:
        return "Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ë²ˆì—­ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    phrases_to_highlight = ", ".join([f"'{w}'" for w in highlight_words])
    
    SYSTEM_INSTRUCTION = '''ë„ˆëŠ” ë²ˆì—­ê°€ì´ë‹¤. ìš”ì²­ëœ ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë§¥ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³ , ì ˆëŒ€ë¡œ ë‹¤ë¥¸ ì„¤ëª…, ì˜µì…˜, ì§ˆë¬¸, ë¶€ê°€ì ì¸ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤. ì˜¤ì§ ìµœì¢… ë²ˆì—­ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•œë‹¤.'''

    if phrases_to_highlight:
        translation_prompt = f"""
        **ë°˜ë“œì‹œ ì•„ë˜ ëŸ¬ì‹œì•„ì–´ ë‹¨ì–´/êµ¬ì˜ í•œêµ­ì–´ ë²ˆì—­ì´ ë“±ì¥í•˜ë©´, ê·¸ í•œêµ­ì–´ ë²ˆì—­ ë‹¨ì–´/êµ¬ë¥¼ `<PHRASE_START>`ì™€ `<PHRASE_END>` ë§ˆí¬ì—…ìœ¼ë¡œ ê°ì‹¸ì•¼ í•´.**

        ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸: '{russian_text}'
        ë§ˆí¬ì—… ëŒ€ìƒ ëŸ¬ì‹œì•„ì–´ ë‹¨ì–´/êµ¬: {phrases_to_highlight}
        """
    else:
        translation_prompt = f"ì›ë³¸ ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸: '{russian_text}'"

    try:
        res = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=translation_prompt,
            config={"system_instruction": SYSTEM_INSTRUCTION}
        )
        translated = res.text.strip()
        
        # í›„ì²˜ë¦¬: ë§ˆí¬ì—…ì„ HTML Span íƒœê·¸ë¡œ ë³€í™˜
        selected_class = "word-selected"
        translated = translated.replace("<PHRASE_START>", f'<span class="{selected_class}">')
        translated = translated.replace("<PHRASE_END>", '</span>')

        return translated

    except Exception as e:
        return f"ë²ˆì—­ ì˜¤ë¥˜ ë°œìƒ: {e}"


# ---------------------- 3. ì „ì—­ ìŠ¤íƒ€ì¼ ì •ì˜ (í°íŠ¸ ë° ìœ íŠœë¸Œ ë°˜ì‘í˜• CSS í¬í•¨) ----------------------

st.markdown("""
<style>
    /* í°íŠ¸ ì ìš©: Nanum Gothic ì›¹ í°íŠ¸ (UI ê¸€ì”¨ì²´ ë³€ê²½) */
    @import url('[https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&display=swap](https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@400;700&display=swap)');
    
    html, body, .stApp {
        font-family: 'Nanum Gothic', sans-serif !important; 
    }
    
    /* YouTube ë¹„ë””ì˜¤ë¥¼ ìœ„í•œ ë°˜ì‘í˜• ì»¨í…Œì´ë„ˆ */
    .video-responsive {
        overflow: hidden;
        padding-bottom: 56.25%; /* 16:9 ë¹„ìœ¨ (9 / 16 * 100) */
        position: relative;
        height: 0;
    }
    .video-responsive iframe {
        left: 0;
        top: 0;
        height: 100%;
        width: 100%;
        position: absolute;
    }
    .video-container-wrapper {
        margin-top: 15px;
        margin-bottom: 15px;
    }
    
    /* í…ìŠ¤íŠ¸ ì˜ì—­ ê°€ë…ì„± */
    .text-container {
        line-height: 2.0;
        margin-bottom: 20px;
        font-size: 1.25em;
    }
    /* ì„ íƒ/ê²€ìƒ‰ëœ ë‹¨ì–´/êµ¬ í•˜ì´ë¼ì´íŒ… + ë°‘ì¤„ */
    .word-selected {
        color: #007bff !important;
        font-weight: bold;
        background-color: #e0f0ff;
        padding: 2px 0px;
        border-bottom: 3px solid #007bff;
        border-radius: 2px;
    }
    .search-link-container {
        display: flex;
        gap: 10px;
        margin-top: 15px;
        flex-wrap: wrap;
    }
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton>button {
        background-color: #f0f2f6;
        color: #333;
        border: 1px solid #ccc;
        border-radius: 0.5rem;
    }
    .stButton>button:hover {
        background-color: #e8e8e8;
        border-color: #aaa;
    }
    /* ê¸°íƒ€ UI ì¡°ì • */
    .main .stImage {
        padding: 0;
        margin: 0;
    }
    .st-emotion-cache-1215r6w {
        margin-top: 0rem !important;
        padding-top: 0rem !important;
    }
</style>
""", unsafe_allow_html=True)


# ---------------------- 4. ë²„íŠ¼ í´ë¦­ ì‹œ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ëŠ” ì½œë°± í•¨ìˆ˜ ì •ì˜ ----------------------
def load_default_text():
    st.session_state.input_text_area = NEW_DEFAULT_TEXT
    st.session_state.translated_text = ""
    st.session_state.selected_words = []
    st.session_state.clicked_word = None
    st.session_state.word_info = {}
    st.session_state.current_search_query = ""
    st.session_state.last_processed_query = ""


# ---------------------- 5. í•˜ì´ë¼ì´íŒ… ë¡œì§ í•¨ìˆ˜ ì •ì˜ ----------------------
def get_highlighted_html(text_to_process, highlight_words):
    selected_class = "word-selected"
    display_html = text_to_process
    
    highlight_candidates = sorted(
        [word for word in highlight_words if word.strip()],
        key=len,
        reverse=True
    )

    for phrase in highlight_candidates:
        escaped_phrase = re.escape(phrase)
        
        if ' ' in phrase:
            # êµ¬(Phrase) ê²€ìƒ‰
            display_html = re.sub(
                f'({escaped_phrase})',
                f'<span class="{selected_class}">\\1</span>',
                display_html
            )
        else:
            # ë‹¨ì–´(Word) ê²€ìƒ‰ (\bëŠ” ë‹¨ì–´ ê²½ê³„)
            pattern = re.compile(r'\b' + escaped_phrase + r'\b')
            display_html = pattern.sub(
                f'<span class="{selected_class}">{phrase}</span>',
                display_html
            )
    
    return f'<div class="text-container">{display_html}</div>'


# ---------------------- 6. UI ë°°ì¹˜ ë° ë©”ì¸ ë¡œì§ ----------------------

# --- 6.1. OCR ë° í…ìŠ¤íŠ¸ ì…ë ¥ ì„¹ì…˜ ---
st.subheader("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ(ì—…ë°ì´íŠ¸ ì˜ˆì •)")
uploaded_file = st.file_uploader("JPG, PNG ë“± ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    image_bytes = uploaded_file.getvalue()
    ocr_result = detect_text_from_image(image_bytes)
    
    if ocr_result and not ocr_result.startswith(("OCR API í‚¤", "Vision API ì˜¤ë¥˜")):
        st.session_state.ocr_output_text = ocr_result
        st.session_state.input_text_area = ocr_result
        st.session_state.translated_text = ""
        st.success("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
    else:
        st.error(ocr_result)

# í…ìŠ¤íŠ¸ ë°˜ì˜ ë²„íŠ¼ ì¶”ê°€
st.button(
    "ì¤‘ê¸‰ëŸ¬ì‹œì•„ì–´ì—°ìŠµ í…ìŠ¤íŠ¸ ë°˜ì˜í•˜ê¸°(êµì¬ 2ê¶Œ 44í˜ì´ì§€)",
    on_click=load_default_text,
    help="êµì¬ ì—°ìŠµìš© í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ì°½ì— ë°˜ì˜í•©ë‹ˆë‹¤."
)

st.subheader("ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸")
current_text = st.text_area(
    "ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ìœ„ì— ì—…ë¡œë“œëœ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”",
    value=st.session_state.input_text_area,
    height=150,
    key="input_text_area"
)


# í…ìŠ¤íŠ¸ê°€ ìˆ˜ì •ë˜ë©´ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ë²ˆì—­/ë¶„ì„ ìƒíƒœ ì´ˆê¸°í™”
if current_text != st.session_state.last_processed_text:
    st.session_state.translated_text = ""
    st.session_state.selected_words = []
    st.session_state.clicked_word = None
    st.session_state.word_info = {}
    st.session_state.current_search_query = ""


# --- 6.2. ë‹¨ì–´ ê²€ìƒ‰ì°½ ë° ë¡œì§ ---
st.divider()
st.subheader("ğŸ” ë‹¨ì–´/êµ¬ ê²€ìƒ‰")
manual_input = st.text_input("ë‹¨ì–´ ë˜ëŠ” êµ¬ë¥¼ ì…ë ¥í•˜ê³  Enter (ì˜ˆ: 'Ğ¸Ğ´Ñ‘Ñ‚ Ğ¿Ğ¾ ÑƒĞ»Ğ¸Ñ†Ğµ')", key="current_search_query")

if manual_input and manual_input != st.session_state.get("last_processed_query"):
    if manual_input not in st.session_state.selected_words:
        st.session_state.selected_words.append(manual_input)
    
    st.session_state.clicked_word = manual_input
    
    with st.spinner(f"'{manual_input}'ì— ëŒ€í•œ ì •ë³´ ë¶„ì„ ì¤‘..."):
        clean_input = manual_input
        lemma = lemmatize_ru(clean_input)
        pos = get_pos_ru(clean_input)
        try:
            info = fetch_from_gemini(clean_input, lemma, pos)
            # ê¸°ë³¸í˜•(lemma) ê¸°ì¤€ìœ¼ë¡œ ì •ë³´ ì €ì¥. ë‹¨, í˜„ì¬ ê²€ìƒ‰ì–´(token)ê°€ ë‹¤ë¥´ë©´ ì—…ë°ì´íŠ¸
            if lemma not in st.session_state.word_info or st.session_state.word_info.get(lemma, {}).get('loaded_token') != clean_input:
                st.session_state.word_info[lemma] = {**info, "loaded_token": clean_input, "pos": pos}
        except Exception as e:
            st.error(f"Gemini ì˜¤ë¥˜: {e}")
            
    st.session_state.last_processed_query = manual_input

st.markdown("---")


# ---------------------- 7. í…ìŠ¤íŠ¸ í•˜ì´ë¼ì´íŒ… ë° ìƒì„¸ ì •ë³´ ë ˆì´ì•„ì›ƒ ----------------------

# ë ˆì´ì•„ì›ƒ: ì™¼ìª½(ì›ë¬¸), ì˜¤ë¥¸ìª½(ìƒì„¸ì •ë³´ + ì˜ìƒ)
left, right = st.columns([2, 1])


with left:
    st.subheader("ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ ì›ë¬¸")
    
    # --- TTS ë²„íŠ¼ ë° ê°•ì„¸ ë§í¬ ---
    col_tts, col_accent = st.columns([1, 2])
    
    with col_tts:
        ELEVENLABS_URL = "https://elevenlabs.io/"
        st.markdown(
            f"[â–¶ï¸ í…ìŠ¤íŠ¸ ìŒì„± ë“£ê¸° (ElevenLabs)]({ELEVENLABS_URL})",
            unsafe_allow_html=False
        )

    with col_accent:
        ACCENT_ONLINE_URL = "[https://russiangram.com/](https://russiangram.com/)"
        
        st.markdown(
            f"ğŸ”Š [ê°•ì„¸ í‘œì‹œ ì‚¬ì´íŠ¸ë¡œ ì´ë™ (russiangram.com)]({ACCENT_ONLINE_URL})",
            unsafe_allow_html=False
        )
        st.info("â¬†ï¸ ìŒì„± ë“£ê¸° ë° ê°•ì„¸ í™•ì¸ì„ ìœ„í•´ ì™¸ë¶€ ì‚¬ì´íŠ¸ ë§í¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìƒˆ íƒ­ìœ¼ë¡œ ì—´ë¦½ë‹ˆë‹¤.")


    # ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ í•˜ì´ë¼ì´íŒ… ì¶œë ¥ (current_text ì‚¬ìš©)
    ru_html = get_highlighted_html(current_text, st.session_state.selected_words)
    st.markdown(ru_html, unsafe_allow_html=True)
    
    st.markdown("---")


    # ì´ˆê¸°í™” ë²„íŠ¼
    def reset_all_state():
        st.session_state.selected_words = []
        st.session_state.clicked_word = None
        st.session_state.word_info = {}
        st.session_state.current_search_query = ""
        st.session_state.input_text_area = DEFAULT_TEST_TEXT
        st.session_state.ocr_output_text = ""
        st.session_state.translated_text = ""
        st.session_state.last_processed_text = ""


    st.button("ì„ íƒ ë° ê²€ìƒ‰ ì´ˆê¸°í™”", key="reset_button", on_click=reset_all_state)
    

# ---------------------- 7.2. ë‹¨ì–´ ìƒì„¸ ì •ë³´ (right ì»¬ëŸ¼) + ì˜ìƒ ì‚½ì… ----------------------
with right:
    st.subheader("ë‹¨ì–´ ìƒì„¸ ì •ë³´")
    
    current_token = st.session_state.clicked_word
    
    if current_token:
        clean_token = current_token
        lemma = lemmatize_ru(clean_token)
        info = st.session_state.word_info.get(lemma, {})

        if info and "ko_meanings" in info:
            pos = info.get("pos", "í’ˆì‚¬")
            aspect_pair = info.get("aspect_pair")
            
            # --- 1. êµ¬ ì „ì²´ì˜ ì •ë³´ í‘œì‹œ ---
            st.markdown(f"### **{clean_token}**")
            
            if pos == 'ë™ì‚¬' and aspect_pair:
                st.markdown(f"**ê¸°ë³¸í˜• (ë¶ˆì™„ë£Œìƒ):** *{aspect_pair.get('imp', lemma)}*")
                st.markdown(f"**ì™„ë£Œìƒ:** *{aspect_pair.get('perf', 'ì •ë³´ ì—†ìŒ')}*")
                st.markdown(f"**í’ˆì‚¬:** {pos}")
            elif pos == 'êµ¬ í˜•íƒœ': 
                st.markdown(f"**êµ¬(å¥) í˜•íƒœ:** *{lemma}*")
                st.markdown(f"**í’ˆì‚¬:** {pos} (ê°œë³„ ë‹¨ì–´ ë¶„ì„ì„ ì°¸ê³ í•˜ì„¸ìš”)")
            else:
                st.markdown(f"**ê¸°ë³¸í˜• (Lemma):** *{lemma}* ({pos})")
            
            st.divider()

            ko_meanings = info.get("ko_meanings", [])
            examples = info.get("examples", [])

            if ko_meanings:
                st.markdown("#### í•œêµ­ì–´ ëœ»")
                for m in ko_meanings:
                    st.markdown(f"- **{m}**")

            if examples:
                st.markdown("#### ğŸ“– ì˜ˆë¬¸")
                for ex in examples:
                    st.markdown(f"- {ex.get('ru', '')}")
                    st.markdown(f"â€ƒâ†’ {ex.get('ko', '')}")
            else:
                if ko_meanings and ko_meanings[0].startswith(f"'{current_token}'ì˜ API í‚¤ ì—†ìŒ"):
                    st.warning("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì˜ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                elif ko_meanings and ko_meanings[0] == "JSON íŒŒì‹± ì˜¤ë¥˜":
                    st.error("Gemini API ì •ë³´ ì˜¤ë¥˜.")
                else:
                    st.info("ì˜ˆë¬¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # --- 2. êµ¬ ì•ˆì— ìˆëŠ” ê°œë³„ ë‹¨ì–´ ì •ë³´ í‘œì‹œ (ìš”ì²­ ì‚¬í•­ ë°˜ì˜: ê°„ëµ ëœ» ë¡œë“œ) ---
            if pos == 'êµ¬ í˜•íƒœ':
                st.markdown("---")
                st.markdown("#### ë‚±ë§(í† í°) ë¶„ì„")
                
                individual_words = clean_token.split() 
                
                for word in individual_words:
                    # ë¬¸ì¥ë¶€í˜¸ ì œê±° í›„ ì²˜ë¦¬ (ì›í˜• ì¶”ì¶œ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•¨)
                    processed_word = re.sub(r'[.,!?;:"]', '', word) 
                    
                    if not processed_word:
                        continue
                        
                    token_lemma = lemmatize_ru(processed_word)
                    token_pos = get_pos_ru(processed_word)
                    token_info = st.session_state.word_info.get(token_lemma)
                    
                    # ìºì‹œì— ì •ë³´ê°€ ì—†ê±°ë‚˜ êµ¬ ì •ë³´ë§Œ ìˆì„ ê²½ìš°, Gemini APIë¥¼ í˜¸ì¶œí•˜ì—¬ ëœ»ë§Œ ê°€ì ¸ì˜´
                    if not token_info or token_info.get('pos') == 'êµ¬ í˜•íƒœ':
                        try:
                            # ê¸°ë³¸í˜•(lemma)ë§Œìœ¼ë¡œ API í˜¸ì¶œí•˜ì—¬ ê°„ëµ ì •ë³´ë¥¼ ê°€ì ¸ì˜´
                            loaded_info = fetch_from_gemini(token_lemma, token_lemma, token_pos)
                            
                            if loaded_info.get("ko_meanings") and loaded_info["ko_meanings"][0] != "JSON íŒŒì‹± ì˜¤ë¥˜":
                                # ê°€ì ¸ì˜¨ ì •ë³´ë¥¼ ìºì‹œì— ì €ì¥
                                st.session_state.word_info[token_lemma] = {
                                    **loaded_info, 
                                    "loaded_token": token_lemma, 
                                    "pos": token_pos
                                }
                                token_info = st.session_state.word_info[token_lemma]
                            else:
                                st.markdown(f"**{word}** (`{token_lemma}`) â†’ ëœ» ì •ë³´ ë¡œë“œ ì‹¤íŒ¨")
                                continue
                        except Exception as e:
                            st.markdown(f"**{word}** (`{token_lemma}`) â†’ API ì˜¤ë¥˜")
                            continue

                    # *ê°„ëµí™”ëœ ë‹¨ì–´ ë¶„ì„ ê²°ê³¼ë¥¼ í‘œì‹œ*
                    if token_info:
                        token_pos = token_info.get("pos", "í’ˆì‚¬")
                        token_meanings = token_info.get("ko_meanings", [])
                        
                        display_meaning = "; ".join(token_meanings[:1])
                        
                        # ìš”ì²­í•˜ì‹  ê°„ëµí•œ ì¶œë ¥ í˜•ì‹
                        st.markdown(f"**{word}** (`{token_lemma}` - {token_pos}) â†’ **{display_meaning}**")
                    
            # --- 3. ì™¸ë¶€ ê²€ìƒ‰ ë§í¬ ---
            st.markdown("---")
            encoded_query = urllib.parse.quote(clean_token)
            
            multitran_url = f"[https://www.multitran.com/m.exe?s=](https://www.multitran.com/m.exe?s=){encoded_query}&l1=1&l2=2"
            corpus_url = f"[http://search.ruscorpora.ru/search.xml?text=](http://search.ruscorpora.ru/search.xml?text=){encoded_query}&env=alpha&mode=main&sort=gr_tagging&lang=ru&nodia=1"
            
            st.markdown("#### ğŸŒ ì™¸ë¶€ ê²€ìƒ‰")
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown(f"[Multitran ê²€ìƒ‰]({multitran_url})")
            
            with col2:
                st.markdown(f"[êµ­ë¦½ ì½”í¼ìŠ¤ ê²€ìƒ‰]({corpus_url})")
            
        else:
            st.warning("ë‹¨ì–´ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì´ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    else:
        st.info("ê²€ìƒ‰ì°½ì— ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ë©´ ì—¬ê¸°ì— ìƒì„¸ ì •ë³´ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
        
    # --- 10. í™ë³´ ì˜ìƒ ì‚½ì… (ë‹¨ì–´ ìƒì„¸ ì •ë³´ ì„¹ì…˜ì˜ ë§¨ ì•„ë˜) ---
    st.markdown("---")
    st.subheader("ğŸ¬ í”„ë¡œì íŠ¸ í™ë³´ ì˜ìƒ")
    if YOUTUBE_VIDEO_ID:
        video_html = youtube_embed_html(YOUTUBE_VIDEO_ID) 
        st.markdown(video_html, unsafe_allow_html=True)
        st.caption(f"YouTube ì˜ìƒ ID: {YOUTUBE_VIDEO_ID}") 
    else:
        st.warning("í™ë³´ ì˜ìƒì„ í‘œì‹œí•˜ë ¤ë©´ YOUTUBE_VIDEO_IDë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")


# ---------------------- 8. í•˜ë‹¨: ëˆ„ì  ëª©ë¡ + CSV ----------------------
st.divider()
# ë¬¸êµ¬ ìˆ˜ì • ë°˜ì˜
st.subheader("ë‹¨ì–´ ëª©ë¡ (ê¸°ë³¸í˜• ê¸°ì¤€)")

selected = st.session_state.selected_words
word_info = st.session_state.word_info

if word_info:
    rows = []
    processed_lemmas = set()
    
    for tok in selected:
        clean_tok = tok
        lemma = lemmatize_ru(clean_tok)
        if lemma not in processed_lemmas and lemma in word_info:
            info = word_info[lemma]
            if info.get("ko_meanings") and info["ko_meanings"][0] != "JSON íŒŒì‹± ì˜¤ë¥˜":
                pos = info.get("pos", "í’ˆì‚¬")
                
                if pos == 'ë™ì‚¬' and info.get("aspect_pair"):
                    imp = info['aspect_pair'].get('imp', lemma)
                    perf = info['aspect_pair'].get('perf', 'ì •ë³´ ì—†ìŒ')
                    base_form = f"{imp} / {perf}"
                else:
                    base_form = lemma

                short = "; ".join(info["ko_meanings"][:2])
                short = f"({pos}) {short}"

                rows.append({"ê¸°ë³¸í˜•": base_form, "ëŒ€í‘œ ëœ»": short})
                processed_lemmas.add(lemma)

    if rows:
        df = pd.DataFrame(rows)
        st.dataframe(df, hide_index=True)
    else:
        st.info("ì„ íƒëœ ë‹¨ì–´ì˜ ì •ë³´ê°€ ë¡œë“œ ì¤‘ì´ê±°ë‚˜, í‘œì‹œí•  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")


# ---------------------- 9. í•˜ë‹¨: í•œêµ­ì–´ ë²ˆì—­ë³¸ ----------------------
st.divider()
st.subheader("í•œêµ­ì–´ ë²ˆì—­ë³¸")

if st.session_state.translated_text == "" or current_text != st.session_state.last_processed_text:
    st.session_state.translated_text = translate_text(
        current_text,
        st.session_state.selected_words
    )
    st.session_state.last_processed_text = current_text

translated_text = st.session_state.translated_text

if translated_text.startswith("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€"):
    st.error(translated_text)
elif translated_text.startswith("ë²ˆì—­ ì˜¤ë¥˜ ë°œìƒ"):
    st.error(translated_text)
else:
    st.markdown(f'<div class="text-container" style="color: #333; font-weight: 500;">{translated_text}</div>', unsafe_allow_html=True)


# ---------------------- 11. ì €ì‘ê¶Œ í‘œì‹œ (í˜ì´ì§€ ìµœí•˜ë‹¨) ----------------------
st.markdown("---")
st.markdown("""
<div style="text-align: center; font-size: 0.75em; color: #888;">
    ì´ í˜ì´ì§€ëŠ” ì—°ì„¸ëŒ€í•™êµ ë…¸ì–´ë…¸ë¬¸í•™ê³¼ 25-2 ëŸ¬ì‹œì•„ì–´ êµìœ¡ë¡  5íŒ€ì˜ í”„ë¡œì íŠ¸ ê²°ê³¼ë¬¼ì…ë‹ˆë‹¤.
    <br>
    ë³¸ í˜ì´ì§€ì˜ ë‚´ìš©, ê¸°ëŠ¥ ë° ë°ì´í„°ë¥¼ í•™ìŠµ ëª©ì  ì´ì™¸ì˜ ìš©ë„ë¡œ ë¬´ë‹¨ ë³µì œ, ë°°í¬, ìƒì—…ì  ì´ìš©í•  ê²½ìš°,
    ê´€ë ¨ ë²•ë ¹ì— ë”°ë¼ ë¯¼ì‚¬ìƒ ì†í•´ë°°ìƒ ì²­êµ¬ ë° í˜•ì‚¬ìƒ ì²˜ë²Œì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
</div>
""", unsafe_allow_html=True)
