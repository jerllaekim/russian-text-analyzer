import streamlit as st
import re
import os
import json
import pandas as pd
from pymystem3 import Mystem
from google import genai
from google.cloud import vision 
import io
import urllib.parse 

# ---------------------- 0. ì´ˆê¸° ì„¤ì • ë° ì„¸ì…˜ ìƒíƒœ ----------------------
st.set_page_config(page_title="ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ê¸°", layout="wide")
st.title("ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ê¸°") 

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "selected_words" not in st.session_state:
    st.session_state.selected_words = []
if "clicked_word" not in st.session_state:
    st.session_state.clicked_word = None
if "word_info" not in st.session_state:
    st.session_state.word_info = {}
if "current_search_query" not in st.session_state:
    st.session_state.current_search_query = ""
if "ocr_output_text" not in st.session_state:
    st.session_state.ocr_output_text = ""
if "display_text" not in st.session_state:
    st.session_state.display_text = "Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¸Ğ´Ñ‘Ñ‚ Ğ¿Ğ¾ ÑƒĞ»Ğ¸Ñ†Ğµ. Ğ­Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°. Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾. Ğ¯ Ñ‡Ğ°ÑÑ‚Ğ¾ Ñ‡Ğ¸Ñ‚Ğ°Ñ ÑÑ‚Ñƒ ĞºĞ½Ğ¸Ğ³Ñƒ."
if "translated_text" not in st.session_state:
    st.session_state.translated_text = ""
if "last_processed_text" not in st.session_state:
    st.session_state.last_processed_text = "" # â— ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ: ì •í™•í•œ ë³€ìˆ˜ëª…
if "last_processed_query" not in st.session_state:
    st.session_state.last_processed_query = ""


mystem = Mystem()

# ---------------------- í’ˆì‚¬ ë³€í™˜ ë”•ì…”ë„ˆë¦¬ ë° Mystem í•¨ìˆ˜ ----------------------
POS_MAP = {
    'S': 'ëª…ì‚¬', 'V': 'ë™ì‚¬', 'A': 'í˜•ìš©ì‚¬', 'ADV': 'ë¶€ì‚¬', 'PR': 'ì „ì¹˜ì‚¬',
    'CONJ': 'ì ‘ì†ì‚¬', 'INTJ': 'ê°íƒ„ì‚¬', 'PART': 'ë¶ˆë³€í™”ì‚¬', 'NUM': 'ìˆ˜ì‚¬',
    'APRO': 'ëŒ€ëª…ì‚¬ì  í˜•ìš©ì‚¬', 'ANUM': 'ì„œìˆ˜ì‚¬', 'SPRO': 'ëŒ€ëª…ì‚¬',
}

@st.cache_data(show_spinner=False)
def lemmatize_ru(word: str) -> str:
    if ' ' in word.strip():
        return word.strip()
    if re.fullmatch(r'\w+', word, flags=re.UNICODE):
        lemmas = mystem.lemmatize(word)
        return (lemmas[0] if lemmas else word).strip()
    return word

@st.cache_data(show_spinner=False)
def get_pos_ru(word: str) -> str:
    if ' ' in word.strip():
        return 'ê´€ìš©êµ¬'
    if re.fullmatch(r'\w+', word, flags=re.UNICODE):
        analysis = mystem.analyze(word)
        if analysis and 'analysis' in analysis[0] and analysis[0]['analysis']:
            grammar_info = analysis[0]['analysis'][0]['gr']
            pos_abbr = grammar_info.split('=')[0].split(',')[0].strip()
            return POS_MAP.get(pos_abbr, 'í’ˆì‚¬')
    return 'í’ˆì‚¬'

# ---------------------- OCR í•¨ìˆ˜ ----------------------
@st.cache_data(show_spinner="ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘")
def detect_text_from_image(image_bytes):
    try:
        # GCP SA í‚¤ ì„¤ì • (Streamlit Secrets ì‚¬ìš©)
        if st.secrets.get("GCP_SA_KEY"):
            with open("temp_sa_key.json", "w") as f:
                json.dump(st.secrets["GCP_SA_KEY"], f)
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "temp_sa_key.json"
        elif "GOOGLE_APPLICATION_CREDENTIALS" not in os.environ:
            return "OCR API í‚¤(GOOGLE_APPLICATION_CREDENTIALS)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Cloud Vision API ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

        client = vision.ImageAnnotatorClient()
        image = vision.Image(content=image_bytes)
        response = client.text_detection(image=image)
        texts = response.text
        
        if response.error.message:
            return f"Vision API ì˜¤ë¥˜: {response.error.message}"
            
        return texts.split('\n', 1)[0] if texts else "ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    except Exception as e:
        return f"OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


# ---------------------- 1. Gemini ì—°ë™ í•¨ìˆ˜ ----------------------

def get_gemini_client():
    api_key = st.secrets.get("GEMINI_API_KEY", os.getenv("GEMINI_API_KEY"))
    return genai.Client(api_key=api_key) if api_key else None

@st.cache_data(show_spinner=False)
def fetch_from_gemini(word, lemma, pos):
    client = get_gemini_client()
    if not client:
        return {"ko_meanings": [f"'{word}'ì˜ API í‚¤ ì—†ìŒ (GEMINI_API_KEY ì„¤ì • í•„ìš”)"], "examples": []}
    
    SYSTEM_PROMPT = "ë„ˆëŠ” ëŸ¬ì‹œì•„ì–´-í•œêµ­ì–´ í•™ìŠµ ë„ìš°ë¯¸ì´ë‹¤. ëŸ¬ì‹œì•„ì–´ ë‹¨ì–´ì— ëŒ€í•´ ê°„ë‹¨í•œ í•œêµ­ì–´ ëœ»ê³¼ ì˜ˆë¬¸ì„ ìµœëŒ€ ë‘ ê°œë§Œ ì œê³µí•œë‹¤. ë§Œì•½ ë™ì‚¬(V)ì´ë©´, ë¶ˆì™„ë£Œìƒ(imp)ê³¼ ì™„ë£Œìƒ(perf) í˜•íƒœë¥¼ í•¨ê»˜ ì œê³µí•´ì•¼ í•œë‹¤. ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•œë‹¤."
    
    if pos == 'ë™ì‚¬':
        prompt = f"""{SYSTEM_PROMPT}
ë‹¨ì–´: {word}
ê¸°ë³¸í˜•: {lemma}
{{ "ko_meanings": ["ëœ»1", "ëœ»2"], "aspect_pair": {{"imp": "ë¶ˆì™„ë£Œìƒ ë™ì‚¬", "perf": "ì™„ë£Œìƒ ë™ì‚¬"}}, "examples": [ {{"ru": "ì˜ˆë¬¸1", "ko": "ë²ˆì—­1"}}, {{"ru": "ì˜ˆë¬¸2", "ko": "ë²ˆì—­2"}} ] }}
"""
    else:
        prompt = f"""{SYSTEM_PROMPT}
ë‹¨ì–´: {word}
ê¸°ë³¸í˜•: {lemma}
{{ "ko_meanings": ["ëœ»1", "ëœ»2"], "examples": [ {{"ru": "ì˜ˆë¬¸1", "ko": "ë²ˆì—­1"}}, {{"ru": "ì˜ˆë¬¸2", "ko": "ë²ˆì—­2"}} ] }}
"""
    
    res = client.models.generate_content(model="gemini-2.0-flash", contents=prompt)
    text = res.text.strip()
    
    try:
        # JSON íŒŒì‹± ë¡œì§
        if text.startswith("```"):
            text = text.strip("`")
            lines = text.splitlines()
            if lines and lines[0].lower().startswith("json"):
                text = "\n".join(lines[1:])
            elif lines:
                text = "\n".join(lines)
                
        start_index = text.find('{')
        end_index = text.rfind('}')
        
        if start_index != -1 and end_index != -1 and end_index > start_index:
            json_text = text[start_index : end_index + 1]
        else:
            json_text = text
            
        data = json.loads(json_text)
        
        if 'examples' in data and len(data['examples']) > 2:
            data['examples'] = data['examples'][:2]
        return data
        
    except json.JSONDecodeError:
        return {"ko_meanings": ["JSON íŒŒì‹± ì˜¤ë¥˜"], "examples": []}

# ---------------------- 2. í…ìŠ¤íŠ¸ ë²ˆì—­ í•¨ìˆ˜ ----------------------

@st.cache_data(show_spinner="í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ì¤‘...")
def translate_text(russian_text, highlight_words):
    client = get_gemini_client()
    if not client:
        return "Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ë²ˆì—­ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    phrases_to_highlight = ", ".join([f"'{w}'" for w in highlight_words])

    if phrases_to_highlight:
        translation_prompt = f"""
        ë‹¤ìŒ ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë§¥ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì¤˜.
        **ë°˜ë“œì‹œ ì•„ë˜ ëŸ¬ì‹œì•„ì–´ ë‹¨ì–´/êµ¬ì˜ í•œêµ­ì–´ ë²ˆì—­ì´ ë“±ì¥í•˜ë©´, ê·¸ í•œêµ­ì–´ ë²ˆì—­ ë‹¨ì–´/êµ¬ë¥¼ `<PHRASE_START>`ì™€ `<PHRASE_END>` ë§ˆí¬ì—…ìœ¼ë¡œ ê°ì‹¸ì•¼ í•´.**

        ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸: '{russian_text}'
        ë§ˆí¬ì—… ëŒ€ìƒ ëŸ¬ì‹œì•„ì–´ ë‹¨ì–´/êµ¬: {phrases_to_highlight}
        """
    else:
        translation_prompt = f"ë‹¤ìŒ ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë§¥ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì¤˜. ì›ë³¸ í…ìŠ¤íŠ¸: '{russian_text}'"

    try:
        res = client.models.generate_content(
            model="gemini-2.0-flash", 
            contents=translation_prompt
        )
        translated = res.text.strip()
        
        # í›„ì²˜ë¦¬: ë§ˆí¬ì—…ì„ HTML Span íƒœê·¸ë¡œ ë³€í™˜
        selected_class = "word-selected"
        translated = translated.replace("<PHRASE_START>", f'<span class="{selected_class}">')
        translated = translated.replace("<PHRASE_END>", '</span>')

        return translated

    except Exception as e:
        return f"ë²ˆì—­ ì˜¤ë¥˜ ë°œìƒ: {e}"


# ---------------------- 3. ì „ì—­ ìŠ¤íƒ€ì¼ ì •ì˜ ----------------------

st.markdown("""
<style>
    /* í…ìŠ¤íŠ¸ ì˜ì—­ ê°€ë…ì„± */
    .text-container {
        line-height: 2.0;
        margin-bottom: 20px;
        font-size: 1.25em;
    }
    /* ì„ íƒ/ê²€ìƒ‰ëœ ë‹¨ì–´/êµ¬ í•˜ì´ë¼ì´íŒ… + ë°‘ì¤„ */
    .word-selected {
        color: #007bff !important; 
        font-weight: bold;
        background-color: #e0f0ff; 
        padding: 2px 0px;
        border-bottom: 3px solid #007bff; 
        border-radius: 2px;
    }
    .search-link-container {
        display: flex;
        gap: 10px;
        margin-top: 15px;
        flex-wrap: wrap;
    }
</style>
""", unsafe_allow_html=True)


# ---------------------- 4. UI ë°°ì¹˜ ë° ë©”ì¸ ë¡œì§ ----------------------

# --- 4.1. OCR ë° í…ìŠ¤íŠ¸ ì…ë ¥ ì„¹ì…˜ ---
st.subheader("ğŸ–¼ï¸ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ")
uploaded_file = st.file_uploader("JPG, PNG ë“± ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    image_bytes = uploaded_file.getvalue()
    ocr_result = detect_text_from_image(image_bytes)
    
    if ocr_result and not ocr_result.startswith(("OCR API í‚¤", "Vision API ì˜¤ë¥˜")):
        st.session_state.ocr_output_text = ocr_result
        st.session_state.display_text = ocr_result
        st.session_state.translated_text = ""
        st.success("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
    else:
        st.error(ocr_result)


st.subheader("ğŸ“ ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸") 
current_text = st.text_area(
    "ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ìœ„ì— ì—…ë¡œë“œëœ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”", 
    st.session_state.display_text, 
    height=150, 
    key="input_text_area"
)

# í…ìŠ¤íŠ¸ê°€ ìˆ˜ì •ë˜ë©´ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ë²ˆì—­/ë¶„ì„ ìƒíƒœ ì´ˆê¸°í™”
if current_text != st.session_state.display_text:
     st.session_state.display_text = current_text
     st.session_state.translated_text = ""
     st.session_state.selected_words = []
     st.session_state.clicked_word = None
     st.session_state.word_info = {}
     st.session_state.current_search_query = ""

# --- 4.2. ë‹¨ì–´ ê²€ìƒ‰ì°½ ë° ë¡œì§ ---
st.divider()
st.subheader("ğŸ” ë‹¨ì–´/êµ¬ ê²€ìƒ‰") 
manual_input = st.text_input("ë‹¨ì–´ ë˜ëŠ” êµ¬ë¥¼ ì…ë ¥í•˜ê³  Enter (ì˜ˆ: 'Ğ¸Ğ´Ñ‘Ñ‚ Ğ¿Ğ¾ ÑƒĞ»Ğ¸Ñ†Ğµ')", key="current_search_query")

if manual_input and manual_input != st.session_state.get("last_processed_query"):
    if manual_input not in st.session_state.selected_words:
        st.session_state.selected_words.append(manual_input)
    
    st.session_state.clicked_word = manual_input
    
    with st.spinner(f"'{manual_input}'ì— ëŒ€í•œ ì •ë³´ ë¶„ì„ ì¤‘..."):
        lemma = lemmatize_ru(manual_input)
        pos = get_pos_ru(manual_input) 
        try:
            info = fetch_from_gemini(manual_input, lemma, pos)
            if lemma not in st.session_state.word_info or st.session_state.word_info.get(lemma, {}).get('loaded_token') != manual_input:
                st.session_state.word_info[lemma] = {**info, "loaded_token": manual_input, "pos": pos}  
        except Exception as e:
            st.error(f"Gemini ì˜¤ë¥˜: {e}")
        
    st.session_state.last_processed_query = manual_input 

st.markdown("---") 


# ---------------------- 5. í…ìŠ¤íŠ¸ í•˜ì´ë¼ì´íŒ… ë° ìƒì„¸ ì •ë³´ ë ˆì´ì•„ì›ƒ ----------------------

left, right = st.columns([2, 1])

# --- 5.1. í•˜ì´ë¼ì´íŒ… ë¡œì§ (ëŸ¬ì‹œì•„ì–´ ì›ë¬¸) ---
def get_highlighted_html(text_to_process, highlight_words):
    selected_class = "word-selected"
    display_html = text_to_process
    
    highlight_candidates = sorted(
        [word for word in highlight_words if word.strip()],
        key=len,
        reverse=True
    )

    for phrase in highlight_candidates:
        escaped_phrase = re.escape(phrase)
        
        if ' ' in phrase:
            display_html = re.sub(
                f'({escaped_phrase})', 
                f'<span class="{selected_class}">\\1</span>', 
                display_html
            )
        else:
            pattern = re.compile(r'\b' + escaped_phrase + r'\b')
            display_html = pattern.sub(
                f'<span class="{selected_class}">{phrase}</span>', 
                display_html
            )
    
    return f'<div class="text-container">{display_html}</div>'


with left:
    st.subheader("ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ ì›ë¬¸") 
    
    # 1. ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ í•˜ì´ë¼ì´íŒ… ì¶œë ¥
    ru_html = get_highlighted_html(st.session_state.display_text, st.session_state.selected_words)
    st.markdown(ru_html, unsafe_allow_html=True)
    
    # ì´ˆê¸°í™” ë²„íŠ¼
    def reset_all_state():
        st.session_state.selected_words = []
        st.session_state.clicked_word = None
        st.session_state.word_info = {}
        st.session_state.current_search_query = ""
        st.session_state.display_text = "Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¸Ğ´Ñ‘Ñ‚ Ğ¿Ğ¾ ÑƒĞ»Ğ¸Ñ†Ğµ. Ğ­Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°. Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾."
        st.session_state.ocr_output_text = ""
        st.session_state.translated_text = ""
        st.session_state.last_processed_text = ""


    st.markdown("---")
    st.button("ğŸ”„ ì„ íƒ ë° ê²€ìƒ‰ ì´ˆê¸°í™”", key="reset_button", on_click=reset_all_state)
    
    if st.session_state.reset_button:
        st.rerun()

# --- 5.2. ë‹¨ì–´ ìƒì„¸ ì •ë³´ (right ì»¬ëŸ¼) + ê²€ìƒ‰ ë§í¬ ì¶”ê°€ ---
with right:
    st.subheader("ë‹¨ì–´ ìƒì„¸ ì •ë³´")
    
    current_token = st.session_state.clicked_word
    
    if current_token:
        lemma = lemmatize_ru(current_token)
        info = st.session_state.word_info.get(lemma, {})

        if info and "ko_meanings" in info:
            pos = info.get("pos", "í’ˆì‚¬") 
            aspect_pair = info.get("aspect_pair") 
            
            st.markdown(f"### **{current_token}**")
            
            if pos == 'ë™ì‚¬' and aspect_pair:
                st.markdown(f"**ê¸°ë³¸í˜• (ë¶ˆì™„ë£Œìƒ):** *{aspect_pair.get('imp', lemma)}*")
                st.markdown(f"**ì™„ë£Œìƒ:** *{aspect_pair.get('perf', 'ì •ë³´ ì—†ìŒ')}*")
                st.markdown(f"**í’ˆì‚¬:** {pos}")
            elif pos == 'ê´€ìš©êµ¬':
                st.markdown(f"**êµ¬(å¥) í˜•íƒœ:** *{lemma}*")
                st.markdown(f"**í’ˆì‚¬:** {pos}")
            else:
                st.markdown(f"**ê¸°ë³¸í˜• (Lemma):** *{lemma}* ({pos})")
            
            st.divider()

            ko_meanings = info.get("ko_meanings", [])
            examples = info.get("examples", [])

            if ko_meanings:
                st.markdown("#### í•œêµ­ì–´ ëœ»")
                for m in ko_meanings:
                    st.markdown(f"- **{m}**")

            if examples:
                st.markdown("#### ğŸ“– ì˜ˆë¬¸")
                for ex in examples:
                    st.markdown(f"- {ex.get('ru', '')}")
                    st.markdown(f"â€ƒâ†’ {ex.get('ko', '')}")
            else:
                 if ko_meanings and ko_meanings[0].startswith(f"'{current_token}'ì˜ API í‚¤ ì—†ìŒ"):
                    st.warning("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì˜ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                 elif ko_meanings and ko_meanings[0] == "JSON íŒŒì‹± ì˜¤ë¥˜":
                     st.error("Gemini API ì •ë³´ ì˜¤ë¥˜.")
                 else:
                    st.info("ì˜ˆë¬¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # --- ì™¸ë¶€ ê²€ìƒ‰ ë§í¬ ì¶”ê°€ (st.link_button ì‚¬ìš©) ---
            encoded_query = urllib.parse.quote(current_token)
            
            # Multitran: ì˜í•œ ì‚¬ì „ (ê¸°ë³¸)
            multitran_url = f"[https://www.multitran.com/m.exe?s=](https://www.multitran.com/m.exe?s=){encoded_query}&l1=1&l2=2"
            
            # ëŸ¬ì‹œì•„ êµ­ë¦½ ì½”í¼ìŠ¤ (ĞĞšĞ Ğ¯): ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
            corpus_url = f"[http://search.ruscorpora.ru/search.xml?text=](http://search.ruscorpora.ru/search.xml?text=){encoded_query}&env=alpha&mode=main&sort=gr_tagging&lang=ru&nodia=1"
            
            st.markdown("#### ğŸŒ ì™¸ë¶€ ê²€ìƒ‰")
            col1, col2 = st.columns(2)
            with col1:
                st.link_button("ğŸ“š Multitran ê²€ìƒ‰", url=multitran_url)
            with col2:
                st.link_button("ğŸ“– êµ­ë¦½ ì½”í¼ìŠ¤ ê²€ìƒ‰", url=corpus_url)
            
        else:
            st.warning("ë‹¨ì–´ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì´ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    else:
        st.info("ê²€ìƒ‰ì°½ì— ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ë©´ ì—¬ê¸°ì— ìƒì„¸ ì •ë³´ê°€ í‘œì‹œë©ë‹ˆë‹¤.")


# ---------------------- 6. í•˜ë‹¨: ëˆ„ì  ëª©ë¡ + CSV (í•œêµ­ì–´ ë²ˆì—­ë³´ë‹¤ ìœ„ì— ìœ„ì¹˜) ----------------------
st.divider()
st.subheader("ğŸ“ ì„ íƒ ë‹¨ì–´ ëª©ë¡ (ê¸°ë³¸í˜• ê¸°ì¤€)") 

selected = st.session_state.selected_words
word_info = st.session_state.word_info

if word_info:
    rows = []
    processed_lemmas = set()
    
    for tok in selected:
        lemma = lemmatize_ru(tok)
        if lemma not in processed_lemmas and lemma in word_info:
            info = word_info[lemma]
            if info.get("ko_meanings") and info["ko_meanings"][0] != "JSON íŒŒì‹± ì˜¤ë¥˜":
                pos = info.get("pos", "í’ˆì‚¬") 
                
                if pos == 'ë™ì‚¬' and info.get("aspect_pair"):
                    imp = info['aspect_pair'].get('imp', lemma)
                    perf = info['aspect_pair'].get('perf', 'ì •ë³´ ì—†ìŒ')
                    base_form = f"{imp} / {perf}"
                else:
                    base_form = lemma

                short = "; ".join(info["ko_meanings"][:2])
                short = f"({pos}) {short}" 

                rows.append({"ê¸°ë³¸í˜•": base_form, "ëŒ€í‘œ ëœ»": short})
                processed_lemmas.add(lemma)

    if rows:
        df = pd.DataFrame(rows)
        st.dataframe(df, hide_index=True)

        csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
        st.download_button("ğŸ’¾ CSVë¡œ ì €ì¥", csv_bytes, "russian_words.csv", "text/csv")
    else:
        st.info("ì„ íƒëœ ë‹¨ì–´ì˜ ì •ë³´ê°€ ë¡œë“œ ì¤‘ì´ê±°ë‚˜, í‘œì‹œí•  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")


# ---------------------- 7. í•˜ë‹¨: í•œêµ­ì–´ ë²ˆì—­ë³¸ (ê°€ì¥ ì•„ë˜ì— ìœ„ì¹˜) ----------------------
st.divider()
st.subheader("í•œêµ­ì–´ ë²ˆì—­ë³¸") 

# í…ìŠ¤íŠ¸ê°€ ë³€ê²½ë˜ì—ˆê±°ë‚˜ ì•„ì§ ë²ˆì—­ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ìƒˆë¡œ ë²ˆì—­ì„ ìš”ì²­
if st.session_state.translated_text == "" or st.session_state.display_text != st.session_state.last_processed_text:
    st.session_state.translated_text = translate_text(
        st.session_state.display_text, 
        st.session_state.selected_words
    )
    st.session_state.last_processed_text = st.session_state.display_text

translated_text = st.session_state.translated_text

if translated_text.startswith("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€"):
    st.error(translated_text)
elif translated_text.startswith("ë²ˆì—­ ì˜¤ë¥˜ ë°œìƒ"):
    st.error(translated_text)
else:
    st.markdown(f'<div class="text-container" style="color: #333; font-weight: 500;">{translated_text}</div>', unsafe_allow_html=True)

# ---------------------- 8. ì €ì‘ê¶Œ í‘œì‹œ (í˜ì´ì§€ ìµœí•˜ë‹¨) ----------------------
st.markdown("---")
st.markdown("""
<div style="text-align: center; font-size: 0.75em; color: #888;">
    ì´ í˜ì´ì§€ëŠ” ì—°ì„¸ëŒ€í•™êµ ë…¸ì–´ë…¸ë¬¸í•™ê³¼ 25-2 ëŸ¬ì‹œì•„ì–´ êµìœ¡ë¡  5íŒ€ì˜ í”„ë¡œì íŠ¸ ê²°ê³¼ë¬¼ì…ë‹ˆë‹¤. 
    <br>
    ë³¸ í˜ì´ì§€ì˜ ë‚´ìš©, ê¸°ëŠ¥ ë° ë°ì´í„°ë¥¼ í•™ìŠµ ëª©ì  ì´ì™¸ì˜ ìš©ë„ë¡œ ë¬´ë‹¨ ë³µì œ, ë°°í¬, ìƒì—…ì  ì´ìš©í•  ê²½ìš°, 
    ê´€ë ¨ ë²•ë ¹ì— ë”°ë¼ ë¯¼ì‚¬ìƒ ì†í•´ë°°ìƒ ì²­êµ¬ ë° í˜•ì‚¬ìƒ ì²˜ë²Œì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
</div>
""", unsafe_allow_html=True)
