import streamlit as st
import re
import os
import json
import pandas as pd
from pymystem3 import Mystem
from google import genai

# ---------------------- 0. ì´ˆê¸° ì„¤ì • ë° ì„¸ì…˜ ìƒíƒœ ----------------------
st.set_page_config(page_title="ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ê¸°", layout="wide")
st.title("ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ê¸°")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "selected_words" not in st.session_state:
    st.session_state.selected_words = []
if "clicked_word" not in st.session_state:
    st.session_state.clicked_word = None
if "word_info" not in st.session_state:
    st.session_state.word_info = {}

mystem = Mystem()

@st.cache_data(show_spinner=False)
def lemmatize_ru(word: str) -> str:
    """ë‹¨ì–´ì˜ ê¸°ë³¸í˜•(lemma)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if re.fullmatch(r'\w+', word, flags=re.UNICODE):
        lemmas = mystem.lemmatize(word)
        return (lemmas[0] if lemmas else word).strip()
    return word

# ---------------------- 1. Gemini ì—°ë™ í•¨ìˆ˜ ----------------------
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    client = None
else:
    client = genai.Client(api_key=api_key)

SYSTEM_PROMPT = "ë„ˆëŠ” ëŸ¬ì‹œì•„ì–´-í•œêµ­ì–´ í•™ìŠµì„ ë•ëŠ” ë„ìš°ë¯¸ì´ë‹¤. ëŸ¬ì‹œì•„ì–´ ë‹¨ì–´ì— ëŒ€í•´ ê°„ë‹¨í•œ í•œêµ­ì–´ ëœ»ê³¼ ì˜ˆë¬¸ì„ ì œê³µí•œë‹¤. ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•œë‹¤."
def make_prompt(word, lemma):
    return f"""{SYSTEM_PROMPT}
ë‹¨ì–´: {word}
ê¸°ë³¸í˜•: {lemma}
{{ "ko_meanings": ["ëœ»1", "ëœ»2"], "examples": [ {{"ru": "ì˜ˆë¬¸1", "ko": "ë²ˆì—­1"}}, {{"ru": "ì˜ˆë¬¸2", "ko": "ë²ˆì—­2"}} ] }}
"""

@st.cache_data(show_spinner=False)
def fetch_from_gemini(word, lemma):
    if not client:
        return {"ko_meanings": [f"'{word}'ì˜ API í‚¤ ì—†ìŒ (GEMINI_API_KEY ì„¤ì • í•„ìš”)"], "examples": []}
    prompt = make_prompt(word, lemma)
    res = client.models.generate_content(model="gemini-2.0-flash", contents=prompt)
    text = res.text.strip()
    if text.startswith("```"):
        text = text.strip("`")
        lines = text.splitlines()
        if lines and lines[0].lower().startswith("json"):
            text = "\n".join(lines[1:])
        elif lines:
             text = "\n".join(lines)
    return json.loads(text)

# ---------------------- 2. ì „ì—­ ìŠ¤íƒ€ì¼ ì •ì˜ ----------------------

st.markdown("""
<style>
    /* ë‹¨ì–´ ìŠ¤íƒ€ì¼ ì •ì˜ (ë™ì¼) */
    .word-span, .word-selected {
        cursor: pointer;
        padding: 2px 4px;
        margin: 2px 0;
        display: inline-block;
        transition: color 0.2s;
        user-select: none;
        border: none !important;
        text-decoration: none !important; 
        background-color: transparent !important; 
    }
    .word-span:hover {
        color: #007bff;
    }
    .word-selected {
        color: #007bff; 
        font-weight: bold;
    }
    .word-punctuation {
        padding: 2px 0px;
        margin: 2px 0;
        display: inline-block;
        user-select: none;
    }
</style>
""", unsafe_allow_html=True)

# ---------------------- 3. ë©”ì¸ ë¡œì§ ë° ë ˆì´ì•„ì›ƒ ----------------------

text = st.text_area("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", "Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¸Ğ´Ñ‘Ñ‚ Ğ¿Ğ¾ ÑƒĞ»Ğ¸Ñ†Ğµ. Ğ­Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°.")
tokens_with_punct = re.findall(r"(\w+|[^\s\w]+)", text, flags=re.UNICODE)
clickable_words = list(dict.fromkeys([t for t in tokens_with_punct if re.fullmatch(r'\w+', t, flags=re.UNICODE)]))

left, right = st.columns([2, 1])

# ----------------------------------------
# 3.1. ë‹¨ì–´ ëª©ë¡ (left ì»¬ëŸ¼)
# ----------------------------------------
with left:
    st.subheader("ë‹¨ì–´ ëª©ë¡ (í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ)")

    html_all = ""
    for tok in tokens_with_punct:
        if re.fullmatch(r'\w+', tok, flags=re.UNICODE):
            css = "word-span"
            if tok in st.session_state.selected_words:
                css = "word-selected"
            
            # â— onclick: ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•˜ëŠ” JavaScript í•¨ìˆ˜ í˜¸ì¶œ (ë²„íŠ¼ ì œê±°)
            html_all += f"""
            <span class="{css}" onclick="setQueryParam('{tok}');">
                {tok}
            </span>
            """
        else:
            html_all += f"""
            <span class="word-punctuation">
                {tok}
            </span>
            """
    
    st.markdown(html_all, unsafe_allow_html=True)
    
    # ì´ˆê¸°í™” ë²„íŠ¼
    st.markdown("---")
    if st.button("ğŸ”„ ì´ˆê¸°í™”"):
        st.session_state.selected_words = []
        st.session_state.clicked_word = None
        st.session_state.word_info = {}
        st.experimental_set_query_params(word=None) # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
        st.rerun()

# ----------------------------------------
# 3.2. ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ JavaScript ì£¼ì…
# ----------------------------------------

# ì´ JavaScript ì½”ë“œê°€ í´ë¦­ëœ ë‹¨ì–´ë¥¼ URLì˜ 'word' íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì •í•˜ì—¬ Streamlitì„ ì¬ì‹¤í–‰í•©ë‹ˆë‹¤.
st.markdown("""
<script>
    function setQueryParam(word) {
        const url = new URL(window.location.href);
        // 'word' íŒŒë¼ë¯¸í„° ì„¤ì •
        url.searchParams.set('word', word);
        // URLì„ ì—…ë°ì´íŠ¸í•˜ê³ , Streamlitì´ ì´ë¥¼ ê°ì§€í•˜ì—¬ ì¬ì‹¤í–‰ë˜ë„ë¡ í•©ë‹ˆë‹¤.
        window.history.pushState(null, '', url.toString());
        
        // â— Streamlitì´ ì¦‰ì‹œ ì¬ì‹¤í–‰ë˜ë„ë¡ ê°•ì œí•˜ëŠ” í•¨ìˆ˜ í˜¸ì¶œ (ì´ í•¨ìˆ˜ëŠ” Streamlitì˜ ë‚´ë¶€ JSì— í¬í•¨ë˜ì–´ ìˆìŒ)
        if (window.streamlit) {
            window.streamlit.set
        } else {
             // í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ Streamlit ì¬ì‹¤í–‰ ìœ ë„ (ëœ ë¶€ë“œëŸ¬ìš´ ë°©ì‹)
             window.location.reload();
        }
    }
    // â— ìˆ¨ê²¨ì§„ ë²„íŠ¼ ì½”ë“œëŠ” ì™„ì „íˆ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.
</script>
""", unsafe_allow_html=True)


# ----------------------------------------
# 3.3. ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì—ì„œ í´ë¦­ëœ ë‹¨ì–´ ì½ê¸° (ë¡œì§ ì—…ë°ì´íŠ¸)
# ----------------------------------------

query_params = st.experimental_get_query_params()
clicked_word_from_url = query_params.get("word", [None])[0]

# â— URLì—ì„œ ì½ì€ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì„¸ì…˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ì •ë³´ ë¡œë“œ
if clicked_word_from_url and clicked_word_from_url != st.session_state.clicked_word:
    st.session_state.clicked_word = clicked_word_from_url
    tok = clicked_word_from_url
    
    # ë‹¨ì–´ ì •ë³´ ë¡œë“œ ë¡œì§ (ì´ì „ê³¼ ë™ì¼)
    if tok not in st.session_state.selected_words:
        st.session_state.selected_words.append(tok)
    
    lemma = lemmatize_ru(tok)
    if lemma not in st.session_state.word_info or st.session_state.word_info.get(lemma, {}).get('loaded_token') != tok:
        with st.spinner(f"'{tok}'ì˜ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
            try:
                info = fetch_from_gemini(tok, lemma)
                st.session_state.word_info[lemma] = {**info, "loaded_token": tok} 
            except Exception as e:
                st.error(f"ë‹¨ì–´ ì •ë³´ ë¡œë“œ ì˜¤ë¥˜: {e}")
    # st.rerun()ì€ st.experimental_set_query_paramsë‚˜ window.location.reload()ì— ì˜í•´ ë°œìƒí•˜ë¯€ë¡œ ìƒëµ


# ----------------------------------------
# 3.4. ë‹¨ì–´ ìƒì„¸ ì •ë³´ (right ì»¬ëŸ¼)
# ----------------------------------------
with right:
    st.subheader("ë‹¨ì–´ ìƒì„¸ ì •ë³´")
    
    current_token = st.session_state.clicked_word
    
    if current_token:
        lemma = lemmatize_ru(current_token)
        info = st.session_state.word_info.get(lemma, {})

        if info:
            st.markdown(f"### **{current_token}**")
            st.markdown(f"**ê¸°ë³¸í˜• (Lemma):** *{lemma}*")
            st.divider()

            ko_meanings = info.get("ko_meanings", [])
            examples = info.get("examples", [])

            if ko_meanings:
                st.markdown("#### í•œêµ­ì–´ ëœ»")
                for m in ko_meanings:
                    st.markdown(f"- **{m}**")

            if examples:
                st.markdown("#### ğŸ“– ì˜ˆë¬¸")
                for ex in examples:
                    st.markdown(f"- {ex.get('ru', '')}")
                    st.markdown(f"â€ƒâ†’ {ex.get('ko', '')}")
        else:
            st.warning("ë‹¨ì–´ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì´ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    else:
        st.info("ì™¼ìª½ ë‹¨ì–´ ëª©ë¡ì—ì„œ ë‹¨ì–´ë¥¼ í´ë¦­í•´ì£¼ì„¸ìš”.")

# ---------------------- 4. í•˜ë‹¨: ëˆ„ì  ëª©ë¡ + CSV ----------------------
st.divider()
st.subheader("ğŸ“ ì„ íƒí•œ ë‹¨ì–´ ëª¨ìŒ")

selected = st.session_state.selected_words
word_info = st.session_state.word_info

# (ëˆ„ì  ëª©ë¡ ë° CSV ë¡œì§ ìƒëµ - ë™ì¼)

if word_info:
    rows = []
    processed_lemmas = set()
    for tok in selected:
        lemma = lemmatize_ru(tok)
        if lemma not in processed_lemmas and lemma in word_info:
            info = word_info[lemma]
            short = "; ".join(info["ko_meanings"][:2])
            rows.append({"ê¸°ë³¸í˜•": lemma, "ëŒ€í‘œ ëœ»": short})
            processed_lemmas.add(lemma)

    if rows:
        df = pd.DataFrame(rows)
        st.dataframe(df, hide_index=True)

        csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
        st.download_button("ğŸ’¾ CSVë¡œ ì €ì¥", csv_bytes, "words.csv", "text/csv")
    else:
        st.info("ì„ íƒëœ ë‹¨ì–´ì˜ ì •ë³´ë¥¼ ë¡œë“œ ì¤‘ì´ê±°ë‚˜, í‘œì‹œí•  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")


# ---------------------- 5. ì§ì ‘ ë‹¨ì–´ ê²€ìƒ‰ ----------------------
st.divider()
st.subheader("ğŸ” ì§ì ‘ ë‹¨ì–´ ê²€ìƒ‰")

manual = st.text_input("ë‹¨ì–´ ì§ì ‘ ì…ë ¥", "")

if manual:
    lemma = lemmatize_ru(manual)
    st.markdown(f"**ì…ë ¥ ë‹¨ì–´:** {manual}")
    st.markdown(f"**ê¸°ë³¸í˜•(lemma):** *{lemma}*")

    try:
        info = fetch_from_gemini(manual, lemma)
    except Exception as e:
        st.error(f"Gemini ì˜¤ë¥˜: {e}")
        info = {}

    ko_meanings = info.get("ko_meanings", [])
    examples = info.get("examples", [])

    if ko_meanings:
        st.markdown("**í•œêµ­ì–´ ëœ»:**")
        for m in ko_meanings:
            st.markdown(f"- {m}")

    if examples:
        st.markdown("### ğŸ“– ì˜ˆë¬¸")
        for ex in examples:
            st.markdown(f"- **{ex.get('ru','')}**")
            st.markdown(f"â€ƒâ†’ {ex.get('ko','')}")
