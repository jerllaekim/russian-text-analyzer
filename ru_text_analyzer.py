import streamlit as st
import re
import os
import json
import pandas as pd
from pymystem3 import Mystem
from google import genai
from google.cloud import vision # Vision API ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import io

# ---------------------- 0. ì´ˆê¸° ì„¤ì • ë° ì„¸ì…˜ ìƒíƒœ ----------------------
st.set_page_config(page_title="ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ‡·ğŸ‡º ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ê¸° (OCR í†µí•©)")

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "selected_words" not in st.session_state:
    st.session_state.selected_words = []
if "clicked_word" not in st.session_state:
    st.session_state.clicked_word = None
if "word_info" not in st.session_state:
    st.session_state.word_info = {}
if "current_search_query" not in st.session_state:
    st.session_state.current_search_query = ""
if "ocr_output_text" not in st.session_state:
    st.session_state.ocr_output_text = ""
if "display_text" not in st.session_state:
    st.session_state.display_text = "Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¸Ğ´Ñ‘Ñ‚ Ğ¿Ğ¾ ÑƒĞ»Ğ¸Ñ†Ğµ. Ğ­Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°. Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾. Ğ¯ Ñ‡Ğ°ÑÑ‚Ğ¾ Ñ‡Ğ¸Ñ‚Ğ°Ñ ÑÑ‚Ñƒ ĞºĞ½Ğ¸Ğ³Ñƒ."


mystem = Mystem()

# ---------------------- í’ˆì‚¬ ë³€í™˜ ë”•ì…”ë„ˆë¦¬ (ìƒëµ) ----------------------
# (ì´ ë¶€ë¶„ì€ ì›ë³¸ ì½”ë“œì™€ ë™ì¼í•˜ë¯€ë¡œ ìƒëµí•©ë‹ˆë‹¤.)
POS_MAP = {
    'S': 'ëª…ì‚¬', 'V': 'ë™ì‚¬', 'A': 'í˜•ìš©ì‚¬', 'ADV': 'ë¶€ì‚¬', 'PR': 'ì „ì¹˜ì‚¬',
    'CONJ': 'ì ‘ì†ì‚¬', 'INTJ': 'ê°íƒ„ì‚¬', 'PART': 'ë¶ˆë³€í™”ì‚¬', 'NUM': 'ìˆ˜ì‚¬',
    'APRO': 'ëŒ€ëª…ì‚¬ì  í˜•ìš©ì‚¬', 'ANUM': 'ì„œìˆ˜ì‚¬', 'SPRO': 'ëŒ€ëª…ì‚¬',
}

# ---------------------- OCR í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€) ----------------------

@st.cache_data(show_spinner="ì´ë¯¸ì§€ì—ì„œ ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘...")
def detect_text_from_image(image_bytes):
    """Google Cloud Vision APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê°ì§€í•˜ê³  ì¶”ì¶œí•©ë‹ˆë‹¤."""
    
    # ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ì„¤ì • (Streamlit Secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš© ê¶Œì¥)
    try:
        if st.secrets.get("GCP_SA_KEY"):
            # secretsì—ì„œ í‚¤ íŒŒì¼ì„ ì„ì‹œ ì €ì¥
            with open("temp_sa_key.json", "w") as f:
                json.dump(st.secrets["GCP_SA_KEY"], f)
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "temp_sa_key.json"
        elif "GOOGLE_APPLICATION_CREDENTIALS" not in os.environ:
             # í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ íŒŒì¼ ê²½ë¡œ ì„¤ì •ì´ í•„ìš”í•¨ì„ ì•ˆë‚´
            return "OCR API í‚¤(GOOGLE_APPLICATION_CREDENTIALS)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Cloud Vision API ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

        # Vision API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = vision.ImageAnnotatorClient()
        image = vision.Image(content=image_bytes)
        
        # í…ìŠ¤íŠ¸ ê°ì§€ ìš”ì²­
        response = client.text_detection(image=image)
        texts = response.text
        
        if response.error.message:
            return f"Vision API ì˜¤ë¥˜: {response.error.message}"
            
        # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ë¸”ë¡(ì „ì²´ í…ìŠ¤íŠ¸) ë°˜í™˜
        return texts.split('\n', 1)[0] if texts else "ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    except Exception as e:
        return f"OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


# ---------------------- Mystem ë° Gemini í•¨ìˆ˜ (ì›ë˜ ì½”ë“œì—ì„œ ì¬ì‚¬ìš©) ----------------------
@st.cache_data(show_spinner=False)
def lemmatize_ru(word: str) -> str:
    # (ì›ë˜ ì½”ë“œì™€ ë™ì¼)
    if ' ' in word.strip():
        return word.strip()
    if re.fullmatch(r'\w+', word, flags=re.UNICODE):
        lemmas = mystem.lemmatize(word)
        return (lemmas[0] if lemmas else word).strip()
    return word

@st.cache_data(show_spinner=False)
def get_pos_ru(word: str) -> str:
    # (ì›ë˜ ì½”ë“œì™€ ë™ì¼)
    if ' ' in word.strip():
        return 'ê´€ìš©êµ¬'
    if re.fullmatch(r'\w+', word, flags=re.UNICODE):
        analysis = mystem.analyze(word)
        if analysis and 'analysis' in analysis[0] and analysis[0]['analysis']:
            grammar_info = analysis[0]['analysis'][0]['gr']
            pos_abbr = grammar_info.split('=')[0].split(',')[0].strip()
            return POS_MAP.get(pos_abbr, 'í’ˆì‚¬')
    return 'í’ˆì‚¬'

SYSTEM_PROMPT = "ë„ˆëŠ” ëŸ¬ì‹œì•„ì–´-í•œêµ­ì–´ í•™ìŠµ ë„ìš°ë¯¸ì´ë‹¤. ëŸ¬ì‹œì•„ì–´ ë‹¨ì–´ì— ëŒ€í•´ ê°„ë‹¨í•œ í•œêµ­ì–´ ëœ»ê³¼ ì˜ˆë¬¸ì„ ìµœëŒ€ ë‘ ê°œë§Œ ì œê³µí•œë‹¤. ë§Œì•½ ë™ì‚¬(V)ì´ë©´, ë¶ˆì™„ë£Œìƒ(imp)ê³¼ ì™„ë£Œìƒ(perf) í˜•íƒœë¥¼ í•¨ê»˜ ì œê³µí•´ì•¼ í•œë‹¤. ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•œë‹¤."

def make_prompt(word, lemma, pos):
    # (ì›ë˜ ì½”ë“œì™€ ë™ì¼)
    if pos == 'ë™ì‚¬':
        return f"""{SYSTEM_PROMPT}
ë‹¨ì–´: {word}
ê¸°ë³¸í˜•: {lemma}
{{ "ko_meanings": ["ëœ»1", "ëœ»2"], "aspect_pair": {{"imp": "ë¶ˆì™„ë£Œìƒ ë™ì‚¬", "perf": "ì™„ë£Œìƒ ë™ì‚¬"}}, "examples": [ {{"ru": "ì˜ˆë¬¸1", "ko": "ë²ˆì—­1"}}, {{"ru": "ì˜ˆë¬¸2", "ko": "ë²ˆì—­2"}} ] }}
"""
    else:
        return f"""{SYSTEM_PROMPT}
ë‹¨ì–´: {word}
ê¸°ë³¸í˜•: {lemma}
{{ "ko_meanings": ["ëœ»1", "ëœ»2"], "examples": [ {{"ru": "ì˜ˆë¬¸1", "ko": "ë²ˆì—­1"}}, {{"ru": "ì˜ˆë¬¸2", "ko": "ë²ˆì—­2"}} ] }}
"""

@st.cache_data(show_spinner=False)
def fetch_from_gemini(word, lemma, pos):
    # (ì›ë˜ ì½”ë“œì™€ ë™ì¼, API í‚¤ ì„¤ì • ë¶€ë¶„ì€ st.secretsë¥¼ í†µí•´ í†µí•© ê´€ë¦¬)
    api_key = st.secrets.get("GEMINI_API_KEY", os.getenv("GEMINI_API_KEY"))
    client = genai.Client(api_key=api_key) if api_key else None
    
    if not client:
        return {"ko_meanings": [f"'{word}'ì˜ API í‚¤ ì—†ìŒ (GEMINI_API_KEY ì„¤ì • í•„ìš”)"], "examples": []}
        
    prompt = make_prompt(word, lemma, pos)
    res = client.models.generate_content(model="gemini-2.0-flash", contents=prompt)
    text = res.text.strip()
    
    try:
        # JSON íŒŒì‹± ë¡œì§ (ì›ë˜ ì½”ë“œì™€ ë™ì¼)
        if text.startswith("```"):
            text = text.strip("`")
            lines = text.splitlines()
            if lines and lines[0].lower().startswith("json"):
                text = "\n".join(lines[1:])
            elif lines:
                text = "\n".join(lines)
                
        start_index = text.find('{')
        end_index = text.rfind('}')
        
        if start_index != -1 and end_index != -1 and end_index > start_index:
            json_text = text[start_index : end_index + 1]
        else:
            json_text = text
            
        data = json.loads(json_text)
        
        if 'examples' in data and len(data['examples']) > 2:
            data['examples'] = data['examples'][:2]
        return data
        
    except json.JSONDecodeError:
        # st.error(f"Gemini ì‘ë‹µì„ JSONìœ¼ë¡œ ë””ì½”ë”©í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ í…ìŠ¤íŠ¸ ì‹œì‘: {text[:100]}...")
        return {"ko_meanings": ["JSON íŒŒì‹± ì˜¤ë¥˜"], "examples": []}


# ---------------------- 2. ì „ì—­ ìŠ¤íƒ€ì¼ ì •ì˜ (ë°‘ì¤„ ê°•ì¡° ì¶”ê°€) ----------------------

st.markdown("""
<style>
    /* í…ìŠ¤íŠ¸ ì˜ì—­ ê°€ë…ì„± */
    .text-container {
        line-height: 2.0;
        margin-bottom: 20px;
        font-size: 1.25em;
    }
    /* ì„ íƒ/ê²€ìƒ‰ëœ ë‹¨ì–´/êµ¬ í•˜ì´ë¼ì´íŒ… + ë°‘ì¤„ */
    .word-selected {
        color: #007bff !important; 
        font-weight: bold;
        background-color: #e0f0ff; 
        padding: 2px 0px;
        border-bottom: 3px solid #007bff; /* íŒŒë€ìƒ‰ ë°‘ì¤„ ì¶”ê°€ */
        border-radius: 2px;
    }
    .word-punctuation {
        padding: 0px 0px;
        margin: 0;
        display: inline-block;
        white-space: pre;
        font-size: 1.25em;
    }
</style>
""", unsafe_allow_html=True)


# ---------------------- 3. UI ë°°ì¹˜ ë° ë©”ì¸ ë¡œì§ ----------------------

# --- 3.1. ì´ë¯¸ì§€ ì—…ë¡œë“œ ì„¹ì…˜ (ìƒˆë¡œ ì¶”ê°€) ---
st.subheader("ğŸ–¼ï¸ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)")
uploaded_file = st.file_uploader("JPG, PNG ë“± ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    # íŒŒì¼ì„ ë°”ì´íŠ¸ë¡œ ì½ê¸°
    image_bytes = uploaded_file.getvalue()
    
    # í…ìŠ¤íŠ¸ ê°ì§€ ì‹¤í–‰
    ocr_result = detect_text_from_image(image_bytes)
    
    # ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ ì˜ì—­ì— ë°˜ì˜
    if ocr_result and not ocr_result.startswith(("OCR API í‚¤", "Vision API ì˜¤ë¥˜")):
        st.session_state.ocr_output_text = ocr_result
        st.session_state.display_text = ocr_result # ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        st.success("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
    else:
        st.error(ocr_result)


# --- 3.2. í…ìŠ¤íŠ¸ ì…ë ¥ì°½ (ì—…ë¡œë“œëœ í…ìŠ¤íŠ¸ í¬í•¨) ---
st.subheader("ğŸ“ ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸")
# display_text ìƒíƒœë¥¼ ì‚¬ìš©í•˜ì—¬ OCR ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
current_text = st.text_area(
    "ëŸ¬ì‹œì•„ì–´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ìœ„ì— ì—…ë¡œë“œëœ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”", 
    st.session_state.display_text, 
    height=150, 
    key="input_text_area"
)

# í…ìŠ¤íŠ¸ ì˜ì—­ì´ ìˆ˜ì •ë˜ë©´, display_text ìƒíƒœë„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
if current_text != st.session_state.display_text:
     st.session_state.display_text = current_text
     # í…ìŠ¤íŠ¸ê°€ ë³€ê²½ë˜ë©´ ëª¨ë“  ê²€ìƒ‰ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
     st.session_state.selected_words = []
     st.session_state.clicked_word = None
     st.session_state.word_info = {}
     st.session_state.current_search_query = ""

# --- 3.3. ë‹¨ì–´ ê²€ìƒ‰ì°½ ---
st.divider()
st.subheader("ğŸ” ì§ì ‘ ë‹¨ì–´ ê²€ìƒ‰")
manual_input = st.text_input("ë‹¨ì–´ ì…ë ¥ í›„ Enter (êµ¬ ê²€ìƒ‰ ì‹œ ê³µë°± í¬í•¨ ì…ë ¥)", key="current_search_query")

# ---------------------- 4. ê²€ìƒ‰ ì²˜ë¦¬ ë¡œì§ ----------------------

if manual_input and manual_input != st.session_state.get("last_processed_query"):
    # 1. ê²€ìƒ‰ëœ ë‹¨ì–´ë¥¼ ì„ íƒ ëª©ë¡ì— ì¶”ê°€
    if manual_input not in st.session_state.selected_words:
        st.session_state.selected_words.append(manual_input)
    
    # 2. ìƒì„¸ ì •ë³´ ì˜ì—­ì— í‘œì‹œë  ë‹¨ì–´ ì—…ë°ì´íŠ¸
    st.session_state.clicked_word = manual_input
    
    # ************** ì •ë³´ ë¡œë“œ ë° ì €ì¥ **************
    with st.spinner(f"'{manual_input}'ì— ëŒ€í•œ ì •ë³´ ë¶„ì„ ì¤‘..."):
        lemma = lemmatize_ru(manual_input)
        pos = get_pos_ru(manual_input) # í’ˆì‚¬ ì¶”ì¶œ
        
        try:
            info = fetch_from_gemini(manual_input, lemma, pos)
            
            # ê²€ìƒ‰ëœ ë‹¨ì–´ì˜ ì •ë³´ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥ (í’ˆì‚¬ ì •ë³´ ì¶”ê°€)
            if lemma not in st.session_state.word_info or st.session_state.word_info.get(lemma, {}).get('loaded_token') != manual_input:
                st.session_state.word_info[lemma] = {**info, "loaded_token": manual_input, "pos": pos}  
                
        except Exception as e:
            st.error(f"Gemini ì˜¤ë¥˜: {e}")
        
    st.session_state.last_processed_query = manual_input # ì²˜ë¦¬ ì™„ë£Œëœ ì¿¼ë¦¬ ê¸°ë¡

st.markdown("---") 


# ---------------------- 5. í…ìŠ¤íŠ¸ í•˜ì´ë¼ì´íŒ… ë° ìƒì„¸ ì •ë³´ ë ˆì´ì•„ì›ƒ ----------------------

left, right = st.columns([2, 1])

# --- 5.1. í…ìŠ¤íŠ¸ í•˜ì´ë¼ì´íŒ… (left ì»¬ëŸ¼ - **ìˆ˜ì •ëœ ë¡œì§**) ---
with left:
    st.subheader("ì…ë ¥ëœ í…ìŠ¤íŠ¸ í•˜ì´ë¼ì´íŒ…")
    st.info("ê²€ìƒ‰ì°½ì— ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ë©´ í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ë‹¨ì–´/êµ¬ê°€ í•˜ì´ë¼ì´íŠ¸ë©ë‹ˆë‹¤.")

    selected_class = "word-selected"
    display_text_html = st.session_state.display_text # ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    
    # 1. 'êµ¬' ë‹¨ìœ„ì˜ í•˜ì´ë¼ì´íŒ… ì²˜ë¦¬ë¥¼ ìœ„í•´ ê²€ìƒ‰ëœ ë‹¨ì–´/êµ¬ ëª©ë¡ì„ ì—­ìˆœìœ¼ë¡œ ì •ë ¬ (ê¸´ êµ¬ ìš°ì„  ì²˜ë¦¬)
    highlight_candidates = sorted(
        [word for word in st.session_state.selected_words if word.strip()],
        key=len,
        reverse=True
    )

    # 2. í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ê²€ìƒ‰ëœ êµ¬/ë‹¨ì–´ë¥¼ HTML ë§ˆí¬ì—…ìœ¼ë¡œ ëŒ€ì²´
    for phrase in highlight_candidates:
        # re.escapeë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ë³´í˜¸
        escaped_phrase = re.escape(phrase)
        
        # ë„ì–´ì“°ê¸°ê°€ ìˆëŠ” 'êµ¬'ëŠ” ê·¸ëŒ€ë¡œ ì°¾ê³ , ë‹¨ì¼ ë‹¨ì–´ëŠ” ì •í™•íˆ ì°¾ê¸°
        if ' ' in phrase:
             # 're.sub'ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°˜ë³µ ëŒ€ì²´
            display_text_html = re.sub(
                f'({escaped_phrase})', 
                f'<span class="{selected_class}">\\1</span>', 
                display_text_html
            )
        else:
            # ë‹¨ì–´ ê²½ê³„(\b)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ë‹¨ì–´ ì¼ì¹˜
            # ì´ë¯¸ HTML íƒœê·¸ê°€ ì ìš©ëœ ë¶€ë¶„ì€ ë¬´ì‹œí•˜ë„ë¡ ë¡œì§ì„ ë” ë³µì¡í•˜ê²Œ ë§Œë“¤ í•„ìš”ê°€ ìˆìœ¼ë‚˜,
            # ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ì¼ë‹¨ \bë¡œ ëŒ€ì²´
            pattern = re.compile(r'\b' + escaped_phrase + r'\b')
            display_text_html = pattern.sub(
                f'<span class="{selected_class}">{phrase}</span>', 
                display_text_html
            )
            
    # 3. ìµœì¢… HTML ì¶œë ¥
    final_html = f'<div class="text-container">{display_text_html}</div>'
    st.markdown(final_html, unsafe_allow_html=True)
    
    # ì´ˆê¸°í™” ë²„íŠ¼ì„ ìœ„í•œ ì½œë°± í•¨ìˆ˜ ì •ì˜
    def reset_all_state():
        # (ì›ë˜ ì½”ë“œì™€ ë™ì¼)
        st.session_state.selected_words = []
        st.session_state.clicked_word = None
        st.session_state.word_info = {}
        st.session_state.current_search_query = ""
        st.session_state.display_text = "Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞº Ğ¸Ğ´Ñ‘Ñ‚ Ğ¿Ğ¾ ÑƒĞ»Ğ¸Ñ†Ğµ. Ğ­Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°. Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾." # ê¸°ë³¸ í…ìŠ¤íŠ¸ë¡œ ë³µì›
        st.session_state.ocr_output_text = ""


    st.markdown("---")
    st.button("ğŸ”„ ì„ íƒ ë° ê²€ìƒ‰ ì´ˆê¸°í™”", key="reset_button", on_click=reset_all_state)
    
    if st.session_state.reset_button:
        st.rerun()

# --- 5.2. ë‹¨ì–´ ìƒì„¸ ì •ë³´ (right ì»¬ëŸ¼ - ì›ë˜ ì½”ë“œì™€ ë™ì¼) ---
with right:
    st.subheader("ë‹¨ì–´ ìƒì„¸ ì •ë³´")
    
    current_token = st.session_state.clicked_word
    
    if current_token:
        lemma = lemmatize_ru(current_token)
        info = st.session_state.word_info.get(lemma, {})

        if info and "ko_meanings" in info:
            pos = info.get("pos", "í’ˆì‚¬") 
            aspect_pair = info.get("aspect_pair") 
            
            st.markdown(f"### **{current_token}**")
            
            if pos == 'ë™ì‚¬' and aspect_pair:
                st.markdown(f"**ê¸°ë³¸í˜• (ë¶ˆì™„ë£Œìƒ):** *{aspect_pair.get('imp', lemma)}*")
                st.markdown(f"**ì™„ë£Œìƒ:** *{aspect_pair.get('perf', 'ì •ë³´ ì—†ìŒ')}*")
                st.markdown(f"**í’ˆì‚¬:** {pos}")
            elif pos == 'ê´€ìš©êµ¬':
                st.markdown(f"**êµ¬(å¥) í˜•íƒœ:** *{lemma}*")
                st.markdown(f"**í’ˆì‚¬:** {pos}")
            else:
                st.markdown(f"**ê¸°ë³¸í˜• (Lemma):** *{lemma}* ({pos})")
            
            st.divider()

            ko_meanings = info.get("ko_meanings", [])
            examples = info.get("examples", [])

            if ko_meanings:
                st.markdown("#### í•œêµ­ì–´ ëœ»")
                for m in ko_meanings:
                    st.markdown(f"- **{m}**")

            if examples:
                st.markdown("#### ğŸ“– ì˜ˆë¬¸")
                for ex in examples:
                    st.markdown(f"- {ex.get('ru', '')}")
                    st.markdown(f"â€ƒâ†’ {ex.get('ko', '')}")
            else:
                 if ko_meanings and ko_meanings[0].startswith(f"'{current_token}'ì˜ API í‚¤ ì—†ìŒ"):
                    st.warning("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì˜ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                 elif ko_meanings and ko_meanings[0] == "JSON íŒŒì‹± ì˜¤ë¥˜":
                     st.error("Gemini API ì •ë³´ ì˜¤ë¥˜.")
                 else:
                    st.info("ì˜ˆë¬¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("ë‹¨ì–´ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì´ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    else:
        st.info("ê²€ìƒ‰ì°½ì— ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ë©´ ì—¬ê¸°ì— ìƒì„¸ ì •ë³´ê°€ í‘œì‹œë©ë‹ˆë‹¤.")

# ---------------------- 6. í•˜ë‹¨: ëˆ„ì  ëª©ë¡ + CSV (ì›ë˜ ì½”ë“œì™€ ë™ì¼) ----------------------
st.divider()
st.subheader("ğŸ“ ì„ íƒí•œ ë‹¨ì–´ ëª¨ìŒ (ê¸°ë³¸í˜• ê¸°ì¤€)")

selected = st.session_state.selected_words
word_info = st.session_state.word_info

if word_info:
    rows = []
    processed_lemmas = set()
    
    for tok in selected:
        lemma = lemmatize_ru(tok)
        if lemma not in processed_lemmas and lemma in word_info:
            info = word_info[lemma]
            if info.get("ko_meanings") and info["ko_meanings"][0] != "JSON íŒŒì‹± ì˜¤ë¥˜":
                pos = info.get("pos", "í’ˆì‚¬") 
                
                if pos == 'ë™ì‚¬' and info.get("aspect_pair"):
                    imp = info['aspect_pair'].get('imp', lemma)
                    perf = info['aspect_pair'].get('perf', 'ì •ë³´ ì—†ìŒ')
                    base_form = f"{imp} / {perf}"
                else:
                    base_form = lemma

                short = "; ".join(info["ko_meanings"][:2])
                short = f"({pos}) {short}" 

                rows.append({"ê¸°ë³¸í˜•": base_form, "ëŒ€í‘œ ëœ»": short})
                processed_lemmas.add(lemma)

    if rows:
        df = pd.DataFrame(rows)
        st.dataframe(df, hide_index=True)

        csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
        st.download_button("ğŸ’¾ CSVë¡œ ì €ì¥", csv_bytes, "russian_words.csv", "text/csv")
    else:
        st.info("ì„ íƒëœ ë‹¨ì–´ì˜ ì •ë³´ê°€ ë¡œë“œ ì¤‘ì´ê±°ë‚˜, í‘œì‹œí•  ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")


# ---------------------- 7. ì €ì‘ê¶Œ í‘œì‹œ (í˜ì´ì§€ ìµœí•˜ë‹¨) ----------------------
st.markdown("---")
st.markdown("""
<div style="text-align: center; font-size: 0.75em; color: #888;">
    ì´ í˜ì´ì§€ëŠ” **ì—°ì„¸ëŒ€í•™êµ ë…¸ì–´ë…¸ë¬¸í•™ê³¼ 25-2 ëŸ¬ì‹œì•„ì–´ êµìœ¡ë¡  5íŒ€ì˜ í”„ë¡œì íŠ¸ ê²°ê³¼ë¬¼**ì…ë‹ˆë‹¤. 
    <br>
    ë³¸ í˜ì´ì§€ì˜ ë‚´ìš©, ê¸°ëŠ¥ ë° ë°ì´í„°ë¥¼ **í•™ìŠµ ëª©ì  ì´ì™¸ì˜ ìš©ë„ë¡œ ë¬´ë‹¨ ë³µì œ, ë°°í¬, ìƒì—…ì  ì´ìš©**í•  ê²½ìš°, 
    ê´€ë ¨ ë²•ë ¹ì— ë”°ë¼ **ë¯¼ì‚¬ìƒ ì†í•´ë°°ìƒ ì²­êµ¬ ë° í˜•ì‚¬ìƒ ì²˜ë²Œ**ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
</div>
""", unsafe_allow_html=True)
